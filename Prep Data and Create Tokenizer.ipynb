{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SsdsuHodIQ6F",
    "outputId": "e1bd6e43-7764-49df-a06c-09543d4486d7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "import pickle\n",
    "import progressbar\n",
    "\n",
    "#from tensorflow.python.keras.utils import Progbar\n",
    "\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
    "\n",
    "from text_preprocessing import tokenizer_word\n",
    "from language_model_processing import read_raw_data_preprocess_and_save, create_vocab_df\n",
    "from bpe import create_token_vocabulary, get_stats, merge_vocab, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yu_ObMh3IQ6L"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'master'\n",
    "INPUT_TYPE = 'txt' #Options: tokens, txt, csv\n",
    "TO_SPLIT_CLITICS = True #Set to false if clitics already tokenized\n",
    "DATASET_FILE_MAP = {'all': 'Social_pt.txt'}\n",
    "\n",
    "if DATASET_FILE_MAP.get('all'):\n",
    "    file_split = 'all'\n",
    "else:\n",
    "    file_split = 'split'\n",
    "\n",
    "UNK_TOKEN = None #none if isnt one\n",
    "NUM_MERGES = 30000 #VOCABULARY_SIZE = NUM_MERGES + N_BYTES (~1500)\n",
    "\n",
    "mini_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHkzkh57IQ6O"
   },
   "outputs": [],
   "source": [
    "notebook_dir = pathlib.Path.cwd()\n",
    "repo_dir = notebook_dir\n",
    "(repo_dir / \"models\").mkdir(exist_ok = True)\n",
    "(repo_dir / \"models\" / \"base\").mkdir(exist_ok = True)\n",
    "dataset_dir = repo_dir / \"datasets\" / \"base\" / DATASET_NAME\n",
    "models_dir = repo_dir / \"models\" / \"base\"\n",
    "(models_dir / DATASET_NAME).mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"preprocessed_base_data\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\" / \"train\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\" / \"validate\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"pretraining_base_data\").mkdir(exist_ok = True)\n",
    "processed_data_dir = models_dir / DATASET_NAME / \"processed_base_data\"\n",
    "pretraining_data_dir = models_dir / DATASET_NAME / \"pretraining_base_data\"\n",
    "(models_dir / DATASET_NAME / \"language_maps\").mkdir(exist_ok = True)\n",
    "language_maps_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"language_maps\"\n",
    "(language_maps_dir).mkdir(exist_ok = True)\n",
    "    \n",
    "models_dir = models_dir / DATASET_NAME\n",
    "\n",
    "def save_obj(obj, directory, name):\n",
    "    with open(directory / \"{}.pkl\".format(name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name, directory):\n",
    "    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQRkQY-TIQ6R"
   },
   "source": [
    "# 1. Clean text and build tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AdUkGQ8ZIQ6S"
   },
   "outputs": [],
   "source": [
    "read_raw_data_preprocess_and_save(dataset_file_map=DATASET_FILE_MAP, \n",
    "                                  models_dir=models_dir, \n",
    "                                  dataset_dir=dataset_dir,\n",
    "                                  input_type=INPUT_TYPE,\n",
    "                                  split_clitics=TO_SPLIT_CLITICS,\n",
    "                                  remove_numbers=False,\n",
    "                                  base_folder='preprocessed_base_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRZVvJVuIQ6W",
    "outputId": "1b23dcc6-5b32-4ae8-c77a-4886f78e3d29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (417 of 417) |######################| Elapsed Time: 0:00:08 Time:  0:00:08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41362485"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if file_split == 'all':\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\"\n",
    "else:\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\" / \"train\"\n",
    "\n",
    "corpus = []\n",
    "MAX = None\n",
    "if not MAX:\n",
    "    MAX = len(os.listdir(preprocessed_training_data_dir))\n",
    "\n",
    "with progressbar.ProgressBar(max_value=MAX) as bar:\n",
    "    for i, file in enumerate(os.listdir(preprocessed_training_data_dir)):\n",
    "        with open(os.path.join(preprocessed_training_data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            words = list(reader)[0]\n",
    "            corpus += words\n",
    "        if i == MAX:\n",
    "            break\n",
    "        bar.update(i)\n",
    "    \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cK8Mjpq6IQ6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "Raw size of emoji 3019\n",
      "Raw size of emoji constituents 7255\n",
      "Final size of emoji constituents 1324\n"
     ]
    }
   ],
   "source": [
    "# U+E000..U+F8FF is defined as a private use area so we use for space and unk\n",
    "unk = '[UNK]'\n",
    "spc = chr(int(\"E001\", 16))\n",
    "cls = '[CLS]'\n",
    "sep = '[SEP]'\n",
    "mask = '[MASK]'\n",
    "pad = '[PAD]'\n",
    "\n",
    "id_to_vocab = create_token_vocabulary()\n",
    "unk_id = len(id_to_vocab)\n",
    "spc_id = len(id_to_vocab) + 1\n",
    "cls_id = len(id_to_vocab) + 2\n",
    "sep_id = len(id_to_vocab) + 3\n",
    "mask_id = len(id_to_vocab) + 4\n",
    "pad_id = len(id_to_vocab) + 5\n",
    "\n",
    "id_to_vocab[unk_id] = unk\n",
    "id_to_vocab[spc_id] = spc\n",
    "id_to_vocab[cls_id] = cls\n",
    "id_to_vocab[sep_id] = sep\n",
    "id_to_vocab[mask_id] = mask\n",
    "id_to_vocab[pad_id] = pad\n",
    "\n",
    "save_obj(id_to_vocab, language_maps_dir, \"byte_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTqoiKZfIQ6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE vocab size: 1503\n",
      "letter to id\n",
      "id to letter\n",
      "Total word vocab size 787349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (30000 of 30000) |#################| Elapsed Time: 22:53:53 Time: 22:53:53\n"
     ]
    }
   ],
   "source": [
    "vocab_to_id = {v: i for i, v in id_to_vocab.items()}\n",
    "id_to_vocab = {i: v for v, i in vocab_to_id.items()}  # Reverse as the emoji and other characters have some overlap \n",
    "_ = vocab_to_id.pop(unk)\n",
    "\n",
    "print(\"BPE vocab size:\", len(vocab_to_id))\n",
    "\n",
    "print('letter to id')\n",
    "corpus = [[vocab_to_id.get(l, unk_id) if l is not \" \" else spc_id for l in word] for word in tokenizer_word(corpus,\n",
    "                                                                                                            keep_phrases=False,\n",
    "                                                                                                            tokenize_punc=True,\n",
    "                                                                                                            split_clitics=True,\n",
    "                                                                                                            keep_preceeding_space=True)]\n",
    "print('id to letter')\n",
    "corpus = [\" \".join([id_to_vocab[l] for l in word]) for word in corpus]\n",
    "\n",
    "count_dict = dict(Counter(corpus).most_common())\n",
    "\n",
    "print(\"Total word vocab size\", len(count_dict))\n",
    "bpe_merges = []\n",
    "vocab_to_id_current_max_id = sorted(list(vocab_to_id.values()))[-1]\n",
    "with progressbar.ProgressBar(max_value=NUM_MERGES) as bar:\n",
    "    for i in range(NUM_MERGES):\n",
    "        vocab_to_id_current_max_id += 1\n",
    "        pairs = get_stats(count_dict)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        bpe_merges.append(best)\n",
    "        vocab_to_id[\"\".join(best)] = vocab_to_id_current_max_id\n",
    "        count_dict = merge_vocab(best, count_dict)\n",
    "        bar.update(i)\n",
    "id_to_vocab = {i: v for v, i in vocab_to_id.items()}\n",
    "id_to_vocab[unk_id] = unk\n",
    "\n",
    "save_obj(bpe_merges, language_maps_dir, \"bpe_merges\")\n",
    "save_obj(id_to_vocab, language_maps_dir, \"id_to_vocab\")\n",
    "save_obj(vocab_to_id, language_maps_dir, \"vocab_to_id\")\n",
    "pd.DataFrame(list(vocab_to_id.keys())).to_csv(language_maps_dir / 'vocab_file.csv', encoding='utf-8', header=False, index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGOD1TzwIQ6g",
    "outputId": "7fa99781-c613-4f21-eb3f-929413ca81d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "florete e no sabre . a espada s√≥ foi introduzida na edi√ß√£o seguinte , em 1900 . apenas nos jogos\n",
      "['flor', 'ete', 'e', 'no', 'sabre', '.', 'a', 'espada', 's√≥', 'foi', 'introduzida', 'na', 'edi√ß√£o', 'seguinte', ',', 'em', '1900', '.', 'apenas', 'nos', 'jogos']\n"
     ]
    }
   ],
   "source": [
    "testcase = \" \".join(words[1000:1020])\n",
    "bert_tokenizer = tokenization.FullTokenizer(language_maps_dir)\n",
    "print(testcase)\n",
    "print(bert_tokenizer.tokenize(testcase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L32vj6AZIQ6j",
    "outputId": "326d0c45-07c0-40f2-cdd0-edfb38f5d549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol√° isso √© mais uma BAGUNCA üòÇüòÇüòÇ\n",
      "['ol√°', 'isso', '√©', 'mais', 'uma', 'bagun', 'ca', 'üòÇ', 'üòÇ', 'üòÇ']\n"
     ]
    }
   ],
   "source": [
    "testcase = \"Ol√° isso √© mais uma BAGUNCA üòÇüòÇüòÇ\"\n",
    "print(testcase)\n",
    "print(bert_tokenizer.tokenize(testcase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xkXxL74IQ6o"
   },
   "source": [
    "# Prep data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPVEhY3nIQ6p",
    "outputId": "3f84d4a9-6c71-4f30-925d-3f07d2160110"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (417 of 417) |######################| Elapsed Time: 0:00:06 Time:  0:00:06\n"
     ]
    }
   ],
   "source": [
    "if file_split == 'all':\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\"\n",
    "else:\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\" / \"train\"\n",
    "\n",
    "corpus = []\n",
    "MAX = None\n",
    "if not MAX:\n",
    "    MAX = len(os.listdir(preprocessed_training_data_dir))\n",
    "\n",
    "with progressbar.ProgressBar(max_value=MAX) as bar:\n",
    "    for i, file in enumerate(os.listdir(preprocessed_training_data_dir)):\n",
    "        with open(os.path.join(preprocessed_training_data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            words = list(reader)[0]\n",
    "            corpus += words\n",
    "        if i == MAX:\n",
    "            break\n",
    "        bar.update(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lr6jDXpcIQ6t",
    "outputId": "6212e2ce-e489-4adc-8709-e601d69251f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['status',\n",
       " ',',\n",
       " 'gender',\n",
       " ',',\n",
       " 'and',\n",
       " 'consumer',\n",
       " 'nationalism',\n",
       " 'in',\n",
       " 'south',\n",
       " 'korea',\n",
       " ',',\n",
       " 'columbia',\n",
       " 'university',\n",
       " 'press',\n",
       " ',',\n",
       " '_tk_up_',\n",
       " 'isbn',\n",
       " '0',\n",
       " '-',\n",
       " '231',\n",
       " '-',\n",
       " '11616',\n",
       " '-',\n",
       " '0',\n",
       " '\\n',\n",
       " 'yusuf',\n",
       " ',',\n",
       " 'shahid',\n",
       " ';',\n",
       " 'evenett',\n",
       " ',',\n",
       " 'simon',\n",
       " 'j',\n",
       " '.',\n",
       " ',',\n",
       " 'wu',\n",
       " ',',\n",
       " 'weiping',\n",
       " '.',\n",
       " '2001',\n",
       " 'facets',\n",
       " 'of',\n",
       " 'globalization',\n",
       " 'international',\n",
       " 'and',\n",
       " 'local',\n",
       " 'dimensions',\n",
       " 'of',\n",
       " 'development',\n",
       " 'world',\n",
       " 'bank',\n",
       " 'publications',\n",
       " ',',\n",
       " 'pp',\n",
       " '.',\n",
       " '226',\n",
       " '227',\n",
       " '_tk_up_',\n",
       " 'isbn',\n",
       " '0',\n",
       " '-',\n",
       " '8213',\n",
       " '-',\n",
       " '4742',\n",
       " '-',\n",
       " 'x',\n",
       " '\\n',\n",
       " 'no',\n",
       " ',',\n",
       " 'ch≈èng',\n",
       " '-',\n",
       " 'hy≈èn',\n",
       " '1993',\n",
       " 'public',\n",
       " 'administration',\n",
       " 'and',\n",
       " 'the',\n",
       " 'korean',\n",
       " 'transformation',\n",
       " 'concepts',\n",
       " ',',\n",
       " 'policies',\n",
       " ',',\n",
       " 'and',\n",
       " 'value',\n",
       " 'conflicts',\n",
       " ',',\n",
       " 'kumarian',\n",
       " 'press',\n",
       " ',',\n",
       " '_tk_up_',\n",
       " 'isbn',\n",
       " '1',\n",
       " '-',\n",
       " '56549',\n",
       " '-',\n",
       " '022',\n",
       " '-',\n",
       " '3',\n",
       " '\\n',\n",
       " 'dong',\n",
       " 'pode',\n",
       " 'se',\n",
       " 'referir',\n",
       " 'a',\n",
       " '\\n',\n",
       " 'dong',\n",
       " 'moeda',\n",
       " '\\n',\n",
       " 'dong',\n",
       " 'etnia',\n",
       " '\\n',\n",
       " 'dong',\n",
       " 'divis√£o',\n",
       " 'administrativa',\n",
       " '\\n',\n",
       " 'l√≠ngua',\n",
       " 'dong',\n",
       " '\\n',\n",
       " 'lago',\n",
       " 'dongting',\n",
       " ',',\n",
       " 'tamb√©m',\n",
       " 'chamado',\n",
       " 'dong',\n",
       " '\\n',\n",
       " 'rrok',\n",
       " 'kola',\n",
       " 'mirdita',\n",
       " 'klezna',\n",
       " ',',\n",
       " '28',\n",
       " 'de',\n",
       " 'setembro',\n",
       " 'de',\n",
       " '1939',\n",
       " '√©',\n",
       " 'um',\n",
       " 'arcebispo',\n",
       " 'alban√™s',\n",
       " '.',\n",
       " '\\n',\n",
       " 'tendo',\n",
       " 'nascido',\n",
       " 'em',\n",
       " '28',\n",
       " 'de',\n",
       " 'setembro',\n",
       " 'de',\n",
       " '1939',\n",
       " ',',\n",
       " 'foi',\n",
       " 'ordenado',\n",
       " 'sacerdote',\n",
       " 'em',\n",
       " 'nova',\n",
       " 'iorque',\n",
       " 'em',\n",
       " '2',\n",
       " 'de',\n",
       " 'julho',\n",
       " 'de',\n",
       " '1965',\n",
       " ',',\n",
       " 'pelo',\n",
       " 'mons',\n",
       " '.',\n",
       " 'aleksandar',\n",
       " 'tokiƒá',\n",
       " ',',\n",
       " 'arcebispo',\n",
       " 'de',\n",
       " 'bar',\n",
       " ',',\n",
       " 'em',\n",
       " 'montenegro',\n",
       " ',',\n",
       " 'e',\n",
       " 'foi',\n",
       " 'um',\n",
       " 'padre',\n",
       " 'em',\n",
       " 'uma',\n",
       " 'par√≥quia',\n",
       " 'albanesa',\n",
       " 'no',\n",
       " 'bronx',\n",
       " ',',\n",
       " 'em',\n",
       " 'nova',\n",
       " 'iorque',\n",
       " '.',\n",
       " '\\n',\n",
       " 'em',\n",
       " '1¬∫',\n",
       " 'de',\n",
       " 'julho',\n",
       " 'de',\n",
       " '1986',\n",
       " 'o',\n",
       " 'cardeal',\n",
       " 'john',\n",
       " 'joseph',\n",
       " 'o',\n",
       " \"'connor\",\n",
       " ',',\n",
       " 'arcebispo',\n",
       " 'de',\n",
       " 'nova',\n",
       " 'iorque',\n",
       " ',',\n",
       " 'nomeou',\n",
       " '-',\n",
       " 'o',\n",
       " 'chefe',\n",
       " 'da',\n",
       " 'igreja',\n",
       " 'cat√≥lica',\n",
       " 'albanesa',\n",
       " '.',\n",
       " 'mirdita',\n",
       " 'foi',\n",
       " 'nomeado',\n",
       " 'arcebispo',\n",
       " 'da',\n",
       " 'tirana',\n",
       " '-',\n",
       " 'durr√´s',\n",
       " 'em',\n",
       " '25',\n",
       " 'de',\n",
       " 'dezembro',\n",
       " 'de',\n",
       " '1992',\n",
       " ',',\n",
       " 'e',\n",
       " 'consagrado',\n",
       " 'pelo',\n",
       " 'papa',\n",
       " 'jo√£o',\n",
       " 'paulo',\n",
       " 'ii',\n",
       " 'em',\n",
       " 'pessoa',\n",
       " ',',\n",
       " 'durante',\n",
       " 'sua',\n",
       " 'visita',\n",
       " 'pastoral',\n",
       " '√†',\n",
       " 'alb√¢nia',\n",
       " 'em',\n",
       " '25',\n",
       " 'de',\n",
       " 'abril',\n",
       " 'de',\n",
       " '1993',\n",
       " ',',\n",
       " 'depois',\n",
       " 'de',\n",
       " 'a',\n",
       " 'arquidiocese',\n",
       " 'ficar',\n",
       " 'em',\n",
       " 'sede',\n",
       " 'vacante',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'de',\n",
       " 'quarenta',\n",
       " 'anos',\n",
       " '.',\n",
       " 'com',\n",
       " 'ele',\n",
       " ',',\n",
       " 'o',\n",
       " 'papa',\n",
       " 'consagrou',\n",
       " 'outros',\n",
       " 'tr√™s',\n",
       " 'bispos',\n",
       " 'zef',\n",
       " 'simoni',\n",
       " ',',\n",
       " 'frano',\n",
       " 'illia',\n",
       " 'e',\n",
       " 'robert',\n",
       " 'ashta',\n",
       " '.',\n",
       " 'desta',\n",
       " 'forma',\n",
       " ',',\n",
       " 'a',\n",
       " 'hierarquia',\n",
       " 'cat√≥lica',\n",
       " 'foi',\n",
       " 'restabelecida',\n",
       " 'neste',\n",
       " 'pa√≠s',\n",
       " ',',\n",
       " 'ap√≥s',\n",
       " 'muitos',\n",
       " 'anos',\n",
       " 'de',\n",
       " 'persegui√ß√£o',\n",
       " 'comunista',\n",
       " '.',\n",
       " '\\n',\n",
       " 'ele',\n",
       " '√©',\n",
       " 'o',\n",
       " 'presidente',\n",
       " 'da',\n",
       " 'confer√™ncia',\n",
       " 'episcopal',\n",
       " 'da',\n",
       " 'alb√¢nia',\n",
       " ',',\n",
       " 'bem',\n",
       " 'como',\n",
       " 'o',\n",
       " 'diretor',\n",
       " 'da',\n",
       " 'caritas',\n",
       " 'albanesa',\n",
       " '.',\n",
       " '\\n',\n",
       " 'por',\n",
       " 'sua',\n",
       " 'iniciativa',\n",
       " ',',\n",
       " 'foi',\n",
       " 'constru√≠da',\n",
       " 'a',\n",
       " 'catedral',\n",
       " 'de',\n",
       " 's√£o',\n",
       " 'paulo',\n",
       " 'em',\n",
       " 'tirana',\n",
       " '.',\n",
       " 'a',\n",
       " 'arquitetura',\n",
       " 'triangular',\n",
       " 'do',\n",
       " 'edif√≠cio',\n",
       " ',',\n",
       " 'de',\n",
       " 'acordo',\n",
       " 'com',\n",
       " 'o',\n",
       " 'projeto',\n",
       " 'do',\n",
       " 'arcebispo',\n",
       " ',',\n",
       " 'simboliza',\n",
       " 'a',\n",
       " 'coexist√™ncia',\n",
       " 'do',\n",
       " 'islamismo',\n",
       " ',',\n",
       " 'catolicismo',\n",
       " 'e',\n",
       " 'da',\n",
       " 'igreja',\n",
       " 'ortodoxa',\n",
       " 'na',\n",
       " 'alb√¢nia',\n",
       " '.',\n",
       " 'a',\n",
       " 'primeira',\n",
       " 'missa',\n",
       " 'na',\n",
       " 'nova',\n",
       " 'catedral',\n",
       " 'foi',\n",
       " 'celebrada',\n",
       " 'em',\n",
       " '27',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " 'de',\n",
       " '2002',\n",
       " 'pelo',\n",
       " 'ent√£o',\n",
       " 'secret√°rio',\n",
       " 'de',\n",
       " 'estado',\n",
       " 'do',\n",
       " 'vaticano',\n",
       " ',',\n",
       " 'cardeal',\n",
       " 'angelo',\n",
       " 'sodano',\n",
       " '.',\n",
       " '\\n',\n",
       " 'no',\n",
       " 'dia',\n",
       " 'de',\n",
       " 'natal',\n",
       " 'de',\n",
       " '1999',\n",
       " ',',\n",
       " 'o',\n",
       " 'mons',\n",
       " '.',\n",
       " 'mirdita',\n",
       " 'conheceu',\n",
       " 'o',\n",
       " 'arcebispo',\n",
       " 'ortodoxo',\n",
       " 'anastasios',\n",
       " 'da',\n",
       " 'alb√¢nia',\n",
       " '.',\n",
       " '\\n',\n",
       " 'catolicismo',\n",
       " 'na',\n",
       " 'alb√¢nia',\n",
       " '\\n',\n",
       " 'a',\n",
       " 'prova',\n",
       " 'de',\n",
       " 'velocidade',\n",
       " 'individual',\n",
       " 'masculino',\n",
       " 'do',\n",
       " 'esqui',\n",
       " 'cross',\n",
       " '-',\n",
       " 'country',\n",
       " 'nos',\n",
       " 'jogos',\n",
       " 'ol√≠mpicos',\n",
       " 'de',\n",
       " 'inverno',\n",
       " 'de',\n",
       " '2014',\n",
       " 'ocorreu',\n",
       " 'no',\n",
       " 'dia',\n",
       " '11',\n",
       " 'de',\n",
       " 'fevereiro',\n",
       " 'no',\n",
       " 'centro',\n",
       " 'de',\n",
       " 'esqui',\n",
       " 'cross',\n",
       " '-',\n",
       " 'country',\n",
       " 'e',\n",
       " 'biatlo',\n",
       " 'laura',\n",
       " ',',\n",
       " 'na',\n",
       " 'clareira',\n",
       " 'vermelha',\n",
       " 'em',\n",
       " 's√≥chi',\n",
       " '.',\n",
       " '\\n',\n",
       " 'q',\n",
       " 'qualificado',\n",
       " 'para',\n",
       " 'a',\n",
       " 'pr√≥xima',\n",
       " 'fase',\n",
       " '\\n',\n",
       " 'll',\n",
       " 'perdedor',\n",
       " 'de',\n",
       " 'sorte',\n",
       " '\\n',\n",
       " 'pf',\n",
       " 'photo',\n",
       " 'finish',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '1',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '2',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '3',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '4',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '5',\n",
       " '\\n',\n",
       " 'semifinal',\n",
       " '1',\n",
       " '\\n',\n",
       " 'semifinal',\n",
       " '2',\n",
       " '\\n',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'em',\n",
       " 'coreano',\n",
       " 'ÏïÑÏù¥Ïä§ÌÅ¨Î¶º',\n",
       " '√©',\n",
       " 'uma',\n",
       " 'can√ß√£o',\n",
       " 'de',\n",
       " 'g√™nero',\n",
       " 'dance',\n",
       " '-',\n",
       " 'electronic',\n",
       " 'interpretada',\n",
       " 'pelo',\n",
       " 'girl',\n",
       " 'group',\n",
       " 'sul',\n",
       " '-',\n",
       " 'coreano',\n",
       " 'f',\n",
       " 'x',\n",
       " '.',\n",
       " 'a',\n",
       " 'can√ß√£o',\n",
       " 'foi',\n",
       " 'inclu√≠da',\n",
       " 'como',\n",
       " 'faixa',\n",
       " 'em',\n",
       " 'seu',\n",
       " 'primeiro',\n",
       " 'ep',\n",
       " ',',\n",
       " 'nu',\n",
       " '_tk_up_',\n",
       " 'abo',\n",
       " '.',\n",
       " '\\n',\n",
       " 'ice',\n",
       " 'cream',\n",
       " '√©',\n",
       " 'uma',\n",
       " 'can√ß√£o',\n",
       " 'urbana',\n",
       " ',',\n",
       " 'que',\n",
       " 'pertence',\n",
       " 'ao',\n",
       " 'g√™nero',\n",
       " 'musical',\n",
       " 'dance',\n",
       " '-',\n",
       " 'electronic',\n",
       " '.',\n",
       " 'a',\n",
       " 'can√ß√£o',\n",
       " 'foi',\n",
       " 'composta',\n",
       " 'pelo',\n",
       " 'produtor',\n",
       " 'musical',\n",
       " 'hitchhiker',\n",
       " 'que',\n",
       " 'j√°',\n",
       " 'havia',\n",
       " 'trabalhado',\n",
       " 'com',\n",
       " 'a',\n",
       " 'sm',\n",
       " 'entertainment',\n",
       " 'em',\n",
       " 'v√°rios',\n",
       " 'projetos',\n",
       " ',',\n",
       " 'como',\n",
       " 'um',\n",
       " 'dos',\n",
       " 'seus',\n",
       " 'produtores',\n",
       " '.',\n",
       " 'mais',\n",
       " 'tarde',\n",
       " ',',\n",
       " 'ele',\n",
       " 'comp√¥s',\n",
       " 'e',\n",
       " 'arranjou',\n",
       " 'o',\n",
       " 'single',\n",
       " 'do',\n",
       " 'f',\n",
       " 'x',\n",
       " 'pinocchio',\n",
       " 'danger',\n",
       " 'e',\n",
       " 'a',\n",
       " 'can√ß√£o',\n",
       " 'sweet',\n",
       " 'witches',\n",
       " 'de',\n",
       " 'seu',\n",
       " 'primeiro',\n",
       " '√°lbum',\n",
       " 'de',\n",
       " 'est√∫dio',\n",
       " 'pinocchio',\n",
       " ',',\n",
       " 'e',\n",
       " 'a',\n",
       " 'm√∫sica',\n",
       " 'zig',\n",
       " 'zag',\n",
       " 'de',\n",
       " 'seu',\n",
       " 'segundo',\n",
       " 'ep',\n",
       " 'electric',\n",
       " 'shock',\n",
       " '.',\n",
       " 'a',\n",
       " 'letra',\n",
       " 'foi',\n",
       " 'escrita',\n",
       " 'por',\n",
       " 'kim',\n",
       " 'bumin',\n",
       " 'que',\n",
       " '√©',\n",
       " 'conhecido',\n",
       " 'por',\n",
       " 'trabalhar',\n",
       " 'em',\n",
       " 'colabora√ß√£o',\n",
       " 'com',\n",
       " 'hitchhiker',\n",
       " 'escrevendo',\n",
       " 'letras',\n",
       " 'de',\n",
       " 'quase',\n",
       " 'todas',\n",
       " 'as',\n",
       " 'm√∫sicas',\n",
       " 'produzidas',\n",
       " 'por',\n",
       " 'ele',\n",
       " '.',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'foi',\n",
       " 'inclu√≠do',\n",
       " 'como',\n",
       " 'a',\n",
       " 'terceira',\n",
       " 'faixa',\n",
       " 'no',\n",
       " 'primeiro',\n",
       " 'ep',\n",
       " 'do',\n",
       " 'grupo',\n",
       " 'nu',\n",
       " '_tk_up_',\n",
       " 'abo',\n",
       " ',',\n",
       " 'que',\n",
       " 'foi',\n",
       " 'lan√ßado',\n",
       " 'online',\n",
       " 'e',\n",
       " 'offline',\n",
       " 'em',\n",
       " '3',\n",
       " 'de',\n",
       " 'maio',\n",
       " 'de',\n",
       " '2010',\n",
       " '.',\n",
       " '\\n',\n",
       " 'uma',\n",
       " 'vers√£o',\n",
       " 'remix',\n",
       " 'de',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'foi',\n",
       " 'produzida',\n",
       " 'pelo',\n",
       " 'grupo',\n",
       " 'idiotape',\n",
       " '.',\n",
       " 'ela',\n",
       " 'foi',\n",
       " 'lan√ßado',\n",
       " 'como',\n",
       " 'uma',\n",
       " 'faixa',\n",
       " 'no',\n",
       " 'ep',\n",
       " '10',\n",
       " 'cc',\n",
       " 'x',\n",
       " 'sm',\n",
       " 'seoul',\n",
       " 'melody',\n",
       " ',',\n",
       " 'um',\n",
       " 'projeto',\n",
       " 'de',\n",
       " 'colabora√ß√£o',\n",
       " 'conjunta',\n",
       " 'entre',\n",
       " 'a',\n",
       " 'sm',\n",
       " 'entertainment',\n",
       " 'e',\n",
       " '10',\n",
       " 'corso',\n",
       " 'como',\n",
       " ',',\n",
       " 'lan√ßado',\n",
       " 'em',\n",
       " '27',\n",
       " 'de',\n",
       " 'mar√ßo',\n",
       " 'de',\n",
       " '2013',\n",
       " '.',\n",
       " 'o',\n",
       " 'ep',\n",
       " 'tamb√©m',\n",
       " 'continha',\n",
       " 'remixes',\n",
       " 'de',\n",
       " 'can√ß√µes',\n",
       " 'de',\n",
       " 'sucesso',\n",
       " 'dos',\n",
       " 'companheiros',\n",
       " 'de',\n",
       " 'gravadora',\n",
       " 'do',\n",
       " 'f',\n",
       " 'x',\n",
       " ',',\n",
       " 'incluindo',\n",
       " 'before',\n",
       " 'u',\n",
       " 'go',\n",
       " 'do',\n",
       " '_tk_up_',\n",
       " 'tvxq',\n",
       " ',',\n",
       " 'sexy',\n",
       " ',',\n",
       " 'free',\n",
       " 'single',\n",
       " 'do',\n",
       " 'super',\n",
       " 'junior',\n",
       " ',',\n",
       " 'trick',\n",
       " 'do',\n",
       " 'girls',\n",
       " \"'\",\n",
       " 'generation',\n",
       " 'e',\n",
       " 'hello',\n",
       " 'do',\n",
       " 'shinee',\n",
       " '.',\n",
       " '\\n',\n",
       " 'ana',\n",
       " 'tereza',\n",
       " 'bas√≠lio',\n",
       " 'nasceu',\n",
       " '19',\n",
       " 'de',\n",
       " 'outubro',\n",
       " 'de',\n",
       " '1967',\n",
       " '√©',\n",
       " 'uma',\n",
       " 'ju√≠za',\n",
       " 'e',\n",
       " 'advogada',\n",
       " 'brasileira',\n",
       " '.',\n",
       " '√©',\n",
       " 'ju√≠za',\n",
       " 'substituta',\n",
       " 'do',\n",
       " 'tribunal',\n",
       " 'regional',\n",
       " 'eleitoral',\n",
       " ',',\n",
       " 'no',\n",
       " 'rio',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " ',',\n",
       " 'nomeada',\n",
       " 'pelo',\n",
       " 'presidente',\n",
       " 'luiz',\n",
       " 'in√°cio',\n",
       " 'lula',\n",
       " 'da',\n",
       " 'silva',\n",
       " 'em',\n",
       " '2010',\n",
       " ',',\n",
       " 'e',\n",
       " 'em',\n",
       " '2013',\n",
       " 'pela',\n",
       " 'presidente',\n",
       " 'da',\n",
       " 'rep√∫blica',\n",
       " 'dilma',\n",
       " 'rousseff',\n",
       " '.',\n",
       " '\\n',\n",
       " 'ana',\n",
       " 'bas√≠lio',\n",
       " '√©',\n",
       " 'bacharel',\n",
       " 'em',\n",
       " 'direito',\n",
       " 'pela',\n",
       " 'universidade',\n",
       " 'c√¢ndido',\n",
       " 'mendes',\n",
       " ',',\n",
       " 'no',\n",
       " 'rio',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " 'e',\n",
       " 'p√≥s',\n",
       " 'graduada',\n",
       " 'em',\n",
       " 'direito',\n",
       " 'norte',\n",
       " 'americano',\n",
       " 'pela',\n",
       " 'universidade',\n",
       " 'de',\n",
       " 'wisconsin',\n",
       " ',',\n",
       " 'em',\n",
       " 'wisconsin',\n",
       " '.',\n",
       " '\\n',\n",
       " 'em',\n",
       " '1990',\n",
       " ',',\n",
       " 'inciou',\n",
       " 'sua',\n",
       " 'carreira',\n",
       " 'como',\n",
       " 'advogada',\n",
       " ',',\n",
       " 'atuando',\n",
       " 'em',\n",
       " 'escrit√≥rio',\n",
       " 'de',\n",
       " 'advocacia',\n",
       " 'no',\n",
       " 'brasil',\n",
       " '.',\n",
       " 'foi',\n",
       " 's√≥cia',\n",
       " 'internacional',\n",
       " 'do',\n",
       " 'baker',\n",
       " 'mckenzie',\n",
       " '.',\n",
       " 'de',\n",
       " '2006',\n",
       " 'a',\n",
       " '2008',\n",
       " 'foi',\n",
       " 's√≥cia',\n",
       " 'no',\n",
       " 'escrit√≥rio',\n",
       " 'andrade',\n",
       " 'fichtner',\n",
       " 'advogados',\n",
       " 'e',\n",
       " 'em',\n",
       " '2009',\n",
       " 'abriu',\n",
       " 'com',\n",
       " 'mais',\n",
       " 'quatro',\n",
       " 's√≥cios',\n",
       " 'o',\n",
       " 'escrit√≥rio',\n",
       " 'de',\n",
       " 'advocacia',\n",
       " 'bas√≠lio',\n",
       " 'advogados',\n",
       " '.',\n",
       " '\\n',\n",
       " 'entre',\n",
       " 'os',\n",
       " 'anos',\n",
       " 'de',\n",
       " '2011',\n",
       " 'a',\n",
       " '2013',\n",
       " ',',\n",
       " 'foi',\n",
       " 'diretora',\n",
       " 'da',\n",
       " 'escola',\n",
       " 'judici√°ria',\n",
       " 'eleitoral',\n",
       " ',',\n",
       " 'no',\n",
       " 'rio',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " ',',\n",
       " 'vinculada',\n",
       " '√†',\n",
       " 'presid√™ncia',\n",
       " 'do',\n",
       " 'tribunal',\n",
       " 'regional',\n",
       " 'eleitoral',\n",
       " '.',\n",
       " 'sendo',\n",
       " 'respons√°vel',\n",
       " 'pelas',\n",
       " 'novas',\n",
       " 'diretrizes',\n",
       " 'da',\n",
       " 'institui√ß√£o',\n",
       " 'de',\n",
       " 'formar',\n",
       " ',',\n",
       " 'atualizar',\n",
       " 'e',\n",
       " 'especializar',\n",
       " 'magistrados',\n",
       " ',',\n",
       " 'membros',\n",
       " 'do',\n",
       " 'minist√©rio',\n",
       " 'p√∫blico',\n",
       " 'eleitoral',\n",
       " ',',\n",
       " 'servidores',\n",
       " 'do',\n",
       " 'tribunal',\n",
       " 'regional',\n",
       " 'eleitoral',\n",
       " '_tk_up_',\n",
       " 'tre',\n",
       " 'do',\n",
       " 'rio',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " '.',\n",
       " '\\n',\n",
       " 'em',\n",
       " '2013',\n",
       " ',',\n",
       " 'foi',\n",
       " 'eleita',\n",
       " 'pela',\n",
       " 'chambers',\n",
       " 'e',\n",
       " 'partners',\n",
       " 'como',\n",
       " 'uma',\n",
       " 'das',\n",
       " 'refer√™ncias',\n",
       " 'em',\n",
       " 'arbitragem',\n",
       " 'na',\n",
       " 'am√©rica',\n",
       " 'latina',\n",
       " ',',\n",
       " 'e',\n",
       " 'eleita',\n",
       " 'advogada',\n",
       " 'do',\n",
       " 'ano',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/usherwood/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z5XZwijIQ6x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "corp_str = TreebankWordDetokenizer().detokenize(corpus).replace(' .', '.')\n",
    "for i in range(10):\n",
    "    corp_str = corp_str.replace('. '+str(i), '.'+str(i))\n",
    "\n",
    "corp_list = [x for x in sent_tokenize(corp_str) if x != '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jPZzpTvIQ63",
    "outputId": "bb79e4e5-f2bc-40ce-c3c5-7c4923c2fa70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['status, gender, and consumer nationalism in south korea, columbia university press, _tk_up_ isbn 0 - 231 - 11616 - 0 \\n yusuf, shahid; evenett, simon j., wu, weiping.2001 facets of globalization international and local dimensions of development world bank publications, pp.226 227 _tk_up_ isbn 0 - 8213 - 4742 - x \\n no, ch≈èng - hy≈èn 1993 public administration and the korean transformation concepts, policies, and value conflicts, kumarian press, _tk_up_ isbn 1 - 56549 - 022 - 3 \\n dong pode se referir a \\n dong moeda \\n dong etnia \\n dong divis√£o administrativa \\n l√≠ngua dong \\n lago dongting, tamb√©m chamado dong \\n rrok kola mirdita klezna , 28 de setembro de 1939 √© um arcebispo alban√™s.',\n",
       " 'tendo nascido em 28 de setembro de 1939, foi ordenado sacerdote em nova iorque em 2 de julho de 1965, pelo mons.',\n",
       " 'aleksandar tokiƒá, arcebispo de bar, em montenegro, e foi um padre em uma par√≥quia albanesa no bronx, em nova iorque.',\n",
       " \"em 1¬∫ de julho de 1986 o cardeal john joseph o 'connor, arcebispo de nova iorque, nomeou - o chefe da igreja cat√≥lica albanesa.\",\n",
       " 'mirdita foi nomeado arcebispo da tirana - durr√´s em 25 de dezembro de 1992, e consagrado pelo papa jo√£o paulo ii em pessoa, durante sua visita pastoral √† alb√¢nia em 25 de abril de 1993, depois de a arquidiocese ficar em sede vacante por mais de quarenta anos.',\n",
       " 'com ele, o papa consagrou outros tr√™s bispos zef simoni, frano illia e robert ashta.',\n",
       " 'desta forma, a hierarquia cat√≥lica foi restabelecida neste pa√≠s, ap√≥s muitos anos de persegui√ß√£o comunista.',\n",
       " 'ele √© o presidente da confer√™ncia episcopal da alb√¢nia, bem como o diretor da caritas albanesa.',\n",
       " 'por sua iniciativa, foi constru√≠da a catedral de s√£o paulo em tirana.',\n",
       " 'a arquitetura triangular do edif√≠cio, de acordo com o projeto do arcebispo, simboliza a coexist√™ncia do islamismo, catolicismo e da igreja ortodoxa na alb√¢nia.',\n",
       " 'a primeira missa na nova catedral foi celebrada em 27 de janeiro de 2002 pelo ent√£o secret√°rio de estado do vaticano, cardeal angelo sodano.',\n",
       " 'no dia de natal de 1999, o mons.',\n",
       " 'mirdita conheceu o arcebispo ortodoxo anastasios da alb√¢nia.',\n",
       " 'catolicismo na alb√¢nia \\n a prova de velocidade individual masculino do esqui cross - country nos jogos ol√≠mpicos de inverno de 2014 ocorreu no dia 11 de fevereiro no centro de esqui cross - country e biatlo laura, na clareira vermelha em s√≥chi.',\n",
       " 'q qualificado para a pr√≥xima fase \\n ll perdedor de sorte \\n pf photo finish \\n quartas de final 1 \\n quartas de final 2 \\n quartas de final 3 \\n quartas de final 4 \\n quartas de final 5 \\n semifinal 1 \\n semifinal 2 \\n ice cream em coreano ÏïÑÏù¥Ïä§ÌÅ¨Î¶º √© uma can√ß√£o de g√™nero dance - electronic interpretada pelo girl group sul - coreano f x. a can√ß√£o foi inclu√≠da como faixa em seu primeiro ep, nu _tk_up_ abo.',\n",
       " 'ice cream √© uma can√ß√£o urbana, que pertence ao g√™nero musical dance - electronic.',\n",
       " 'a can√ß√£o foi composta pelo produtor musical hitchhiker que j√° havia trabalhado com a sm entertainment em v√°rios projetos, como um dos seus produtores.',\n",
       " 'mais tarde, ele comp√¥s e arranjou o single do f x pinocchio danger e a can√ß√£o sweet witches de seu primeiro √°lbum de est√∫dio pinocchio, e a m√∫sica zig zag de seu segundo ep electric shock.',\n",
       " 'a letra foi escrita por kim bumin que √© conhecido por trabalhar em colabora√ß√£o com hitchhiker escrevendo letras de quase todas as m√∫sicas produzidas por ele.',\n",
       " 'ice cream foi inclu√≠do como a terceira faixa no primeiro ep do grupo nu _tk_up_ abo, que foi lan√ßado online e offline em 3 de maio de 2010. \\n uma vers√£o remix de ice cream foi produzida pelo grupo idiotape.',\n",
       " \"ela foi lan√ßado como uma faixa no ep 10 cc x sm seoul melody, um projeto de colabora√ß√£o conjunta entre a sm entertainment e 10 corso como, lan√ßado em 27 de mar√ßo de 2013. o ep tamb√©m continha remixes de can√ß√µes de sucesso dos companheiros de gravadora do f x, incluindo before u go do _tk_up_ tvxq, sexy, free single do super junior, trick do girls' generation e hello do shinee.\",\n",
       " 'ana tereza bas√≠lio nasceu 19 de outubro de 1967 √© uma ju√≠za e advogada brasileira.',\n",
       " '√© ju√≠za substituta do tribunal regional eleitoral, no rio de janeiro, nomeada pelo presidente luiz in√°cio lula da silva em 2010, e em 2013 pela presidente da rep√∫blica dilma rousseff.',\n",
       " 'ana bas√≠lio √© bacharel em direito pela universidade c√¢ndido mendes, no rio de janeiro e p√≥s graduada em direito norte americano pela universidade de wisconsin, em wisconsin.',\n",
       " 'em 1990, inciou sua carreira como advogada, atuando em escrit√≥rio de advocacia no brasil.',\n",
       " 'foi s√≥cia internacional do baker mckenzie.',\n",
       " 'de 2006 a 2008 foi s√≥cia no escrit√≥rio andrade fichtner advogados e em 2009 abriu com mais quatro s√≥cios o escrit√≥rio de advocacia bas√≠lio advogados.',\n",
       " 'entre os anos de 2011 a 2013, foi diretora da escola judici√°ria eleitoral, no rio de janeiro, vinculada √† presid√™ncia do tribunal regional eleitoral.',\n",
       " 'sendo respons√°vel pelas novas diretrizes da institui√ß√£o de formar, atualizar e especializar magistrados, membros do minist√©rio p√∫blico eleitoral, servidores do tribunal regional eleitoral _tk_up_ tre do rio de janeiro.',\n",
       " 'em 2013, foi eleita pela chambers e partners como uma das refer√™ncias em arbitragem na am√©rica latina, e eleita advogada do ano pela global awards 2012, nas √°reas de contencioso civil e empresarial.',\n",
       " 'em 2012 foi nomeada presidente da comiss√£o de direito eleitoral do instituto dos advogados do brasil \\n em 2010, o presidente da rep√∫blica luiz in√°cio lula da silva nomeou ana tereza bas√≠lio, ao cargo de ju√≠za substituta do tribunal regional eleitoral do estado do rio de janeiro.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_list[:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYTeqNO9IQ68",
    "outputId": "d4f281a6-7479-4c6d-f31d-d85f09acff45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em 2012 foi nomeada presidente da comiss√£o de direito eleitoral do instituto dos advogados do brasil \n",
      " em 2010, o presidente da rep√∫blica luiz in√°cio lula da silva nomeou ana tereza bas√≠lio, ao cargo de ju√≠za substituta do tribunal regional eleitoral do estado do rio de janeiro.\n",
      "['em', '2012', 'foi', 'nomeada', 'presidente', 'da', 'comiss√£o', 'de', 'direito', 'eleitoral', 'do', 'instituto', 'dos', 'advogados', 'do', 'brasil', 'em', '2010', ',', 'o', 'presidente', 'da', 'rep√∫blica', 'luiz', 'in√°cio', 'lula', 'da', 'silva', 'nomeou', 'ana', 'tere', 'za', 'bas√≠lio', ',', 'ao', 'cargo', 'de', 'ju√≠', 'za', 'substitu', 'ta', 'do', 'tribunal', 'regional', 'eleitoral', 'do', 'estado', 'do', 'rio', 'de', 'janeiro', '.']\n"
     ]
    }
   ],
   "source": [
    "print(corp_list[30])\n",
    "print(bert_tokenizer.tokenize(corp_list[30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7WGWr8RJIQ7A"
   },
   "outputs": [],
   "source": [
    "TRAIN_VAL_SPLIT = .1\n",
    "train_size = int(len(corp_list)*TRAIN_VAL_SPLIT)\n",
    "\n",
    "df_train = pd.DataFrame(corp_list[:train_size])\n",
    "df_val = pd.DataFrame(corp_list[train_size:])\n",
    "df_val.to_csv(processed_data_dir / \"validate\" / \"val.csv\", index=False, header=None, quoting=csv.QUOTE_MINIMAL, encoding='utf-8')\n",
    "\n",
    "step = int(train_size/100)\n",
    "for i in range(100):\n",
    "    df_train[step*i:step*(i+1)].to_csv(processed_data_dir / \"train\" / \"train{}.csv\".format(i), index=False, header=None, quoting=csv.QUOTE_MINIMAL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Prep Data and Create Tokenizer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
