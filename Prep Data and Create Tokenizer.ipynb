{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SsdsuHodIQ6F",
    "outputId": "e1bd6e43-7764-49df-a06c-09543d4486d7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "import pickle\n",
    "import progressbar\n",
    "\n",
    "#from tensorflow.python.keras.utils import Progbar\n",
    "\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
    "\n",
    "from text_preprocessing import tokenizer_word\n",
    "from language_model_processing import read_raw_data_preprocess_and_save, create_vocab_df\n",
    "from bpe import create_token_vocabulary, get_stats, merge_vocab, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yu_ObMh3IQ6L"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'master'\n",
    "INPUT_TYPE = 'txt' #Options: tokens, txt, csv\n",
    "TO_SPLIT_CLITICS = True #Set to false if clitics already tokenized\n",
    "DATASET_FILE_MAP = {'all': 'Social_pt.txt'}\n",
    "\n",
    "if DATASET_FILE_MAP.get('all'):\n",
    "    file_split = 'all'\n",
    "else:\n",
    "    file_split = 'split'\n",
    "\n",
    "UNK_TOKEN = None #none if isnt one\n",
    "NUM_MERGES = 30000 #VOCABULARY_SIZE = NUM_MERGES + N_BYTES (~1500)\n",
    "\n",
    "mini_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHkzkh57IQ6O"
   },
   "outputs": [],
   "source": [
    "notebook_dir = pathlib.Path.cwd()\n",
    "repo_dir = notebook_dir\n",
    "(repo_dir / \"models\").mkdir(exist_ok = True)\n",
    "(repo_dir / \"models\" / \"base\").mkdir(exist_ok = True)\n",
    "dataset_dir = repo_dir / \"datasets\" / \"base\" / DATASET_NAME\n",
    "models_dir = repo_dir / \"models\" / \"base\"\n",
    "(models_dir / DATASET_NAME).mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"preprocessed_base_data\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\" / \"train\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\" / \"validate\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"pretraining_base_data\").mkdir(exist_ok = True)\n",
    "processed_data_dir = models_dir / DATASET_NAME / \"processed_base_data\"\n",
    "pretraining_data_dir = models_dir / DATASET_NAME / \"pretraining_base_data\"\n",
    "(models_dir / DATASET_NAME / \"language_maps\").mkdir(exist_ok = True)\n",
    "language_maps_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"language_maps\"\n",
    "(language_maps_dir).mkdir(exist_ok = True)\n",
    "    \n",
    "models_dir = models_dir / DATASET_NAME\n",
    "\n",
    "def save_obj(obj, directory, name):\n",
    "    with open(directory / \"{}.pkl\".format(name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name, directory):\n",
    "    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQRkQY-TIQ6R"
   },
   "source": [
    "# 1. Clean text and build tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AdUkGQ8ZIQ6S"
   },
   "outputs": [],
   "source": [
    "read_raw_data_preprocess_and_save(dataset_file_map=DATASET_FILE_MAP, \n",
    "                                  models_dir=models_dir, \n",
    "                                  dataset_dir=dataset_dir,\n",
    "                                  input_type=INPUT_TYPE,\n",
    "                                  split_clitics=TO_SPLIT_CLITICS,\n",
    "                                  remove_numbers=False,\n",
    "                                  base_folder='preprocessed_base_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRZVvJVuIQ6W",
    "outputId": "1b23dcc6-5b32-4ae8-c77a-4886f78e3d29"
   },
   "outputs": [],
   "source": [
    "if file_split == 'all':\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\"\n",
    "else:\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\" / \"train\"\n",
    "\n",
    "corpus = []\n",
    "MAX = None\n",
    "if not MAX:\n",
    "    MAX = len(os.listdir(preprocessed_training_data_dir))\n",
    "\n",
    "with progressbar.ProgressBar(max_value=MAX) as bar:\n",
    "    for i, file in enumerate(os.listdir(preprocessed_training_data_dir)):\n",
    "        with open(os.path.join(preprocessed_training_data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            words = list(reader)[0]\n",
    "            corpus += words\n",
    "        if i == MAX:\n",
    "            break\n",
    "        bar.update(i)\n",
    "    \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cK8Mjpq6IQ6a"
   },
   "outputs": [],
   "source": [
    "# U+E000..U+F8FF is defined as a private use area so we use for space and unk\n",
    "unk = '[UNK]'\n",
    "spc = chr(int(\"E001\", 16))\n",
    "cls = '[CLS]'\n",
    "sep = '[SEP]'\n",
    "mask = '[MASK]'\n",
    "pad = '[PAD]'\n",
    "\n",
    "id_to_vocab = create_token_vocabulary()\n",
    "unk_id = len(id_to_vocab)\n",
    "spc_id = len(id_to_vocab) + 1\n",
    "cls_id = len(id_to_vocab) + 2\n",
    "sep_id = len(id_to_vocab) + 3\n",
    "mask_id = len(id_to_vocab) + 4\n",
    "pad_id = len(id_to_vocab) + 5\n",
    "\n",
    "id_to_vocab[unk_id] = unk\n",
    "id_to_vocab[spc_id] = spc\n",
    "id_to_vocab[cls_id] = cls\n",
    "id_to_vocab[sep_id] = sep\n",
    "id_to_vocab[mask_id] = mask\n",
    "id_to_vocab[pad_id] = pad\n",
    "\n",
    "save_obj(id_to_vocab, language_maps_dir, \"byte_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTqoiKZfIQ6d"
   },
   "outputs": [],
   "source": [
    "vocab_to_id = {v: i for i, v in id_to_vocab.items()}\n",
    "id_to_vocab = {i: v for v, i in vocab_to_id.items()}  # Reverse as the emoji and other characters have some overlap \n",
    "_ = vocab_to_id.pop(unk)\n",
    "\n",
    "print(\"BPE vocab size:\", len(vocab_to_id))\n",
    "\n",
    "print('letter to id')\n",
    "corpus = [[vocab_to_id.get(l, unk_id) if l is not \" \" else spc_id for l in word] for word in tokenizer_word(corpus,\n",
    "                                                                                                            keep_phrases=False,\n",
    "                                                                                                            tokenize_punc=True,\n",
    "                                                                                                            split_clitics=True,\n",
    "                                                                                                            keep_preceeding_space=True)]\n",
    "print('id to letter')\n",
    "corpus = [\" \".join([id_to_vocab[l] for l in word]) for word in corpus]\n",
    "\n",
    "count_dict = dict(Counter(corpus).most_common())\n",
    "\n",
    "print(\"Total word vocab size\", len(count_dict))\n",
    "bpe_merges = []\n",
    "vocab_to_id_current_max_id = sorted(list(vocab_to_id.values()))[-1]\n",
    "with progressbar.ProgressBar(max_value=NUM_MERGES) as bar:\n",
    "    for i in range(NUM_MERGES):\n",
    "        vocab_to_id_current_max_id += 1\n",
    "        pairs = get_stats(count_dict)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        bpe_merges.append(best)\n",
    "        vocab_to_id[\"\".join(best)] = vocab_to_id_current_max_id\n",
    "        count_dict = merge_vocab(best, count_dict)\n",
    "        bar.update(i)\n",
    "id_to_vocab = {i: v for v, i in vocab_to_id.items()}\n",
    "id_to_vocab[unk_id] = unk\n",
    "\n",
    "save_obj(bpe_merges, language_maps_dir, \"bpe_merges\")\n",
    "save_obj(id_to_vocab, language_maps_dir, \"id_to_vocab\")\n",
    "save_obj(vocab_to_id, language_maps_dir, \"vocab_to_id\")\n",
    "pd.DataFrame(list(vocab_to_id.keys())).to_csv(language_maps_dir / 'vocab_file.csv', encoding='utf-8', header=False, index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGOD1TzwIQ6g",
    "outputId": "7fa99781-c613-4f21-eb3f-929413ca81d5"
   },
   "outputs": [],
   "source": [
    "testcase = \" \".join(words[1000:1020])\n",
    "bert_tokenizer = tokenization.FullTokenizer(language_maps_dir)\n",
    "print(testcase)\n",
    "print(bert_tokenizer.tokenize(testcase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L32vj6AZIQ6j",
    "outputId": "326d0c45-07c0-40f2-cdd0-edfb38f5d549"
   },
   "outputs": [],
   "source": [
    "testcase = \"OlÃ¡ isso Ã© mais uma BAGUNCA ðŸ˜‚ðŸ˜‚ðŸ˜‚\"\n",
    "print(testcase)\n",
    "print(bert_tokenizer.tokenize(testcase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xkXxL74IQ6o"
   },
   "source": [
    "# Prep data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPVEhY3nIQ6p",
    "outputId": "3f84d4a9-6c71-4f30-925d-3f07d2160110"
   },
   "outputs": [],
   "source": [
    "if file_split == 'all':\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\"\n",
    "else:\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\" / \"train\"\n",
    "\n",
    "corpus = []\n",
    "MAX = None\n",
    "if not MAX:\n",
    "    MAX = len(os.listdir(preprocessed_training_data_dir))\n",
    "\n",
    "with progressbar.ProgressBar(max_value=MAX) as bar:\n",
    "    for i, file in enumerate(os.listdir(preprocessed_training_data_dir)):\n",
    "        with open(os.path.join(preprocessed_training_data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            words = list(reader)[0]\n",
    "            corpus += words\n",
    "        if i == MAX:\n",
    "            break\n",
    "        bar.update(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lr6jDXpcIQ6t",
    "outputId": "6212e2ce-e489-4adc-8709-e601d69251f0"
   },
   "outputs": [],
   "source": [
    "corpus[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z5XZwijIQ6x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "corp_str = TreebankWordDetokenizer().detokenize(corpus).replace(' .', '.')\n",
    "for i in range(10):\n",
    "    corp_str = corp_str.replace('. '+str(i), '.'+str(i))\n",
    "\n",
    "corp_list = [x for x in sent_tokenize(corp_str) if x != '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_list = [x.replace('_tk_doc_', ' _tk_doc_ ') for x in corp_list]\n",
    "corp_list = [x[1:] if x[0] == ' ' else x for x in corp_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jPZzpTvIQ63",
    "outputId": "bb79e4e5-f2bc-40ce-c3c5-7c4923c2fa70"
   },
   "outputs": [],
   "source": [
    "corp_list[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYTeqNO9IQ68",
    "outputId": "d4f281a6-7479-4c6d-f31d-d85f09acff45"
   },
   "outputs": [],
   "source": [
    "print(corp_list[30])\n",
    "print(bert_tokenizer.tokenize(corp_list[30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7WGWr8RJIQ7A"
   },
   "outputs": [],
   "source": [
    "TRAIN_VAL_SPLIT = .1\n",
    "train_size = int(len(corp_list)*TRAIN_VAL_SPLIT)\n",
    "\n",
    "df_train = pd.DataFrame(corp_list[:train_size])\n",
    "df_val = pd.DataFrame(corp_list[train_size:])\n",
    "step = int(len(df_val)/10)\n",
    "for i in range(10):\n",
    "    with open(processed_data_dir / \"validate\" / \"val{}.txt\".format(i), \"w\",encoding=\"utf-8\") as fo:\n",
    "        for l in df_val[step*i:step*(i+1)].values.tolist():\n",
    "            fo.write(l[0]+\"\\n\")\n",
    "step = int(train_size/100)\n",
    "for i in range(100):\n",
    "    with open(processed_data_dir / \"train\" / \"train{}.txt\".format(i), \"w\",encoding=\"utf-8\") as fo:\n",
    "        for l in df_train[step*i:step*(i+1)].values.tolist():\n",
    "            fo.write(l[0]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 10\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train[step*i:step*(i+1)].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Prep Data and Create Tokenizer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
