{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SsdsuHodIQ6F",
    "outputId": "e1bd6e43-7764-49df-a06c-09543d4486d7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "import pickle\n",
    "import progressbar\n",
    "\n",
    "#from tensorflow.python.keras.utils import Progbar\n",
    "\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
    "\n",
    "from text_preprocessing import tokenizer_word\n",
    "from language_model_processing import read_raw_data_preprocess_and_save, create_vocab_df\n",
    "from bpe import create_token_vocabulary, get_stats, merge_vocab, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yu_ObMh3IQ6L"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'master'\n",
    "INPUT_TYPE = 'txt' #Options: tokens, txt, csv\n",
    "TO_SPLIT_CLITICS = True #Set to false if clitics already tokenized\n",
    "DATASET_FILE_MAP = {'all': 'Social_pt.txt'}\n",
    "\n",
    "if DATASET_FILE_MAP.get('all'):\n",
    "    file_split = 'all'\n",
    "else:\n",
    "    file_split = 'split'\n",
    "\n",
    "UNK_TOKEN = None #none if isnt one\n",
    "NUM_MERGES = 30000 #VOCABULARY_SIZE = NUM_MERGES + N_BYTES (~1500)\n",
    "\n",
    "mini_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHkzkh57IQ6O"
   },
   "outputs": [],
   "source": [
    "notebook_dir = pathlib.Path.cwd()\n",
    "repo_dir = notebook_dir\n",
    "(repo_dir / \"models\").mkdir(exist_ok = True)\n",
    "(repo_dir / \"models\" / \"base\").mkdir(exist_ok = True)\n",
    "dataset_dir = repo_dir / \"datasets\" / \"base\" / DATASET_NAME\n",
    "models_dir = repo_dir / \"models\" / \"base\"\n",
    "(models_dir / DATASET_NAME).mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"preprocessed_base_data\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\" / \"train\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\" / \"validate\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"pretraining_base_data\").mkdir(exist_ok = True)\n",
    "processed_data_dir = models_dir / DATASET_NAME / \"processed_base_data\"\n",
    "pretraining_data_dir = models_dir / DATASET_NAME / \"pretraining_base_data\"\n",
    "(models_dir / DATASET_NAME / \"language_maps\").mkdir(exist_ok = True)\n",
    "language_maps_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"language_maps\"\n",
    "(language_maps_dir).mkdir(exist_ok = True)\n",
    "    \n",
    "models_dir = models_dir / DATASET_NAME\n",
    "\n",
    "def save_obj(obj, directory, name):\n",
    "    with open(directory / \"{}.pkl\".format(name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name, directory):\n",
    "    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQRkQY-TIQ6R"
   },
   "source": [
    "# 1. Clean text and build tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AdUkGQ8ZIQ6S"
   },
   "outputs": [],
   "source": [
    "read_raw_data_preprocess_and_save(dataset_file_map=DATASET_FILE_MAP, \n",
    "                                  models_dir=models_dir, \n",
    "                                  dataset_dir=dataset_dir,\n",
    "                                  input_type=INPUT_TYPE,\n",
    "                                  split_clitics=TO_SPLIT_CLITICS,\n",
    "                                  remove_numbers=False,\n",
    "                                  base_folder='preprocessed_base_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRZVvJVuIQ6W",
    "outputId": "1b23dcc6-5b32-4ae8-c77a-4886f78e3d29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (417 of 417) |######################| Elapsed Time: 0:00:08 Time:  0:00:08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41362485"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if file_split == 'all':\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\"\n",
    "else:\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\" / \"train\"\n",
    "\n",
    "corpus = []\n",
    "MAX = None\n",
    "if not MAX:\n",
    "    MAX = len(os.listdir(preprocessed_training_data_dir))\n",
    "\n",
    "with progressbar.ProgressBar(max_value=MAX) as bar:\n",
    "    for i, file in enumerate(os.listdir(preprocessed_training_data_dir)):\n",
    "        with open(os.path.join(preprocessed_training_data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            words = list(reader)[0]\n",
    "            corpus += words\n",
    "        if i == MAX:\n",
    "            break\n",
    "        bar.update(i)\n",
    "    \n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cK8Mjpq6IQ6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "Raw size of emoji 3019\n",
      "Raw size of emoji constituents 7255\n",
      "Final size of emoji constituents 1324\n"
     ]
    }
   ],
   "source": [
    "# U+E000..U+F8FF is defined as a private use area so we use for space and unk\n",
    "unk = '[UNK]'\n",
    "spc = chr(int(\"E001\", 16))\n",
    "cls = '[CLS]'\n",
    "sep = '[SEP]'\n",
    "mask = '[MASK]'\n",
    "pad = '[PAD]'\n",
    "\n",
    "id_to_vocab = create_token_vocabulary()\n",
    "unk_id = len(id_to_vocab)\n",
    "spc_id = len(id_to_vocab) + 1\n",
    "cls_id = len(id_to_vocab) + 2\n",
    "sep_id = len(id_to_vocab) + 3\n",
    "mask_id = len(id_to_vocab) + 4\n",
    "pad_id = len(id_to_vocab) + 5\n",
    "\n",
    "id_to_vocab[unk_id] = unk\n",
    "id_to_vocab[spc_id] = spc\n",
    "id_to_vocab[cls_id] = cls\n",
    "id_to_vocab[sep_id] = sep\n",
    "id_to_vocab[mask_id] = mask\n",
    "id_to_vocab[pad_id] = pad\n",
    "\n",
    "save_obj(id_to_vocab, language_maps_dir, \"byte_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTqoiKZfIQ6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE vocab size: 1503\n",
      "letter to id\n",
      "id to letter\n",
      "Total word vocab size 787349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (30000 of 30000) |#################| Elapsed Time: 22:53:53 Time: 22:53:53\n"
     ]
    }
   ],
   "source": [
    "vocab_to_id = {v: i for i, v in id_to_vocab.items()}\n",
    "id_to_vocab = {i: v for v, i in vocab_to_id.items()}  # Reverse as the emoji and other characters have some overlap \n",
    "_ = vocab_to_id.pop(unk)\n",
    "\n",
    "print(\"BPE vocab size:\", len(vocab_to_id))\n",
    "\n",
    "print('letter to id')\n",
    "corpus = [[vocab_to_id.get(l, unk_id) if l is not \" \" else spc_id for l in word] for word in tokenizer_word(corpus,\n",
    "                                                                                                            keep_phrases=False,\n",
    "                                                                                                            tokenize_punc=True,\n",
    "                                                                                                            split_clitics=True,\n",
    "                                                                                                            keep_preceeding_space=True)]\n",
    "print('id to letter')\n",
    "corpus = [\" \".join([id_to_vocab[l] for l in word]) for word in corpus]\n",
    "\n",
    "count_dict = dict(Counter(corpus).most_common())\n",
    "\n",
    "print(\"Total word vocab size\", len(count_dict))\n",
    "bpe_merges = []\n",
    "vocab_to_id_current_max_id = sorted(list(vocab_to_id.values()))[-1]\n",
    "with progressbar.ProgressBar(max_value=NUM_MERGES) as bar:\n",
    "    for i in range(NUM_MERGES):\n",
    "        vocab_to_id_current_max_id += 1\n",
    "        pairs = get_stats(count_dict)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        bpe_merges.append(best)\n",
    "        vocab_to_id[\"\".join(best)] = vocab_to_id_current_max_id\n",
    "        count_dict = merge_vocab(best, count_dict)\n",
    "        bar.update(i)\n",
    "id_to_vocab = {i: v for v, i in vocab_to_id.items()}\n",
    "id_to_vocab[unk_id] = unk\n",
    "\n",
    "save_obj(bpe_merges, language_maps_dir, \"bpe_merges\")\n",
    "save_obj(id_to_vocab, language_maps_dir, \"id_to_vocab\")\n",
    "save_obj(vocab_to_id, language_maps_dir, \"vocab_to_id\")\n",
    "pd.DataFrame(list(vocab_to_id.keys())).to_csv(language_maps_dir / 'vocab_file.csv', encoding='utf-8', header=False, index=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGOD1TzwIQ6g",
    "outputId": "7fa99781-c613-4f21-eb3f-929413ca81d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "florete e no sabre . a espada só foi introduzida na edição seguinte , em 1900 . apenas nos jogos\n",
      "['flor', 'ete', 'e', 'no', 'sabre', '.', 'a', 'espada', 'só', 'foi', 'introduzida', 'na', 'edição', 'seguinte', ',', 'em', '1900', '.', 'apenas', 'nos', 'jogos']\n"
     ]
    }
   ],
   "source": [
    "testcase = \" \".join(words[1000:1020])\n",
    "bert_tokenizer = tokenization.FullTokenizer(language_maps_dir)\n",
    "print(testcase)\n",
    "print(bert_tokenizer.tokenize(testcase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L32vj6AZIQ6j",
    "outputId": "326d0c45-07c0-40f2-cdd0-edfb38f5d549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá isso é mais uma BAGUNCA 😂😂😂\n",
      "['olá', 'isso', 'é', 'mais', 'uma', 'bagun', 'ca', '😂', '😂', '😂']\n"
     ]
    }
   ],
   "source": [
    "testcase = \"Olá isso é mais uma BAGUNCA 😂😂😂\"\n",
    "print(testcase)\n",
    "print(bert_tokenizer.tokenize(testcase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xkXxL74IQ6o"
   },
   "source": [
    "# Prep data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPVEhY3nIQ6p",
    "outputId": "3f84d4a9-6c71-4f30-925d-3f07d2160110"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (417 of 417) |######################| Elapsed Time: 0:00:06 Time:  0:00:06\n"
     ]
    }
   ],
   "source": [
    "if file_split == 'all':\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\"\n",
    "else:\n",
    "    preprocessed_training_data_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"preprocessed_base_data\" / \"train\"\n",
    "\n",
    "corpus = []\n",
    "MAX = None\n",
    "if not MAX:\n",
    "    MAX = len(os.listdir(preprocessed_training_data_dir))\n",
    "\n",
    "with progressbar.ProgressBar(max_value=MAX) as bar:\n",
    "    for i, file in enumerate(os.listdir(preprocessed_training_data_dir)):\n",
    "        with open(os.path.join(preprocessed_training_data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            words = list(reader)[0]\n",
    "            corpus += words\n",
    "        if i == MAX:\n",
    "            break\n",
    "        bar.update(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lr6jDXpcIQ6t",
    "outputId": "6212e2ce-e489-4adc-8709-e601d69251f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['status',\n",
       " ',',\n",
       " 'gender',\n",
       " ',',\n",
       " 'and',\n",
       " 'consumer',\n",
       " 'nationalism',\n",
       " 'in',\n",
       " 'south',\n",
       " 'korea',\n",
       " ',',\n",
       " 'columbia',\n",
       " 'university',\n",
       " 'press',\n",
       " ',',\n",
       " '_tk_up_',\n",
       " 'isbn',\n",
       " '0',\n",
       " '-',\n",
       " '231',\n",
       " '-',\n",
       " '11616',\n",
       " '-',\n",
       " '0',\n",
       " '\\n',\n",
       " 'yusuf',\n",
       " ',',\n",
       " 'shahid',\n",
       " ';',\n",
       " 'evenett',\n",
       " ',',\n",
       " 'simon',\n",
       " 'j',\n",
       " '.',\n",
       " ',',\n",
       " 'wu',\n",
       " ',',\n",
       " 'weiping',\n",
       " '.',\n",
       " '2001',\n",
       " 'facets',\n",
       " 'of',\n",
       " 'globalization',\n",
       " 'international',\n",
       " 'and',\n",
       " 'local',\n",
       " 'dimensions',\n",
       " 'of',\n",
       " 'development',\n",
       " 'world',\n",
       " 'bank',\n",
       " 'publications',\n",
       " ',',\n",
       " 'pp',\n",
       " '.',\n",
       " '226',\n",
       " '227',\n",
       " '_tk_up_',\n",
       " 'isbn',\n",
       " '0',\n",
       " '-',\n",
       " '8213',\n",
       " '-',\n",
       " '4742',\n",
       " '-',\n",
       " 'x',\n",
       " '\\n',\n",
       " 'no',\n",
       " ',',\n",
       " 'chŏng',\n",
       " '-',\n",
       " 'hyŏn',\n",
       " '1993',\n",
       " 'public',\n",
       " 'administration',\n",
       " 'and',\n",
       " 'the',\n",
       " 'korean',\n",
       " 'transformation',\n",
       " 'concepts',\n",
       " ',',\n",
       " 'policies',\n",
       " ',',\n",
       " 'and',\n",
       " 'value',\n",
       " 'conflicts',\n",
       " ',',\n",
       " 'kumarian',\n",
       " 'press',\n",
       " ',',\n",
       " '_tk_up_',\n",
       " 'isbn',\n",
       " '1',\n",
       " '-',\n",
       " '56549',\n",
       " '-',\n",
       " '022',\n",
       " '-',\n",
       " '3',\n",
       " '\\n',\n",
       " 'dong',\n",
       " 'pode',\n",
       " 'se',\n",
       " 'referir',\n",
       " 'a',\n",
       " '\\n',\n",
       " 'dong',\n",
       " 'moeda',\n",
       " '\\n',\n",
       " 'dong',\n",
       " 'etnia',\n",
       " '\\n',\n",
       " 'dong',\n",
       " 'divisão',\n",
       " 'administrativa',\n",
       " '\\n',\n",
       " 'língua',\n",
       " 'dong',\n",
       " '\\n',\n",
       " 'lago',\n",
       " 'dongting',\n",
       " ',',\n",
       " 'também',\n",
       " 'chamado',\n",
       " 'dong',\n",
       " '\\n',\n",
       " 'rrok',\n",
       " 'kola',\n",
       " 'mirdita',\n",
       " 'klezna',\n",
       " ',',\n",
       " '28',\n",
       " 'de',\n",
       " 'setembro',\n",
       " 'de',\n",
       " '1939',\n",
       " 'é',\n",
       " 'um',\n",
       " 'arcebispo',\n",
       " 'albanês',\n",
       " '.',\n",
       " '\\n',\n",
       " 'tendo',\n",
       " 'nascido',\n",
       " 'em',\n",
       " '28',\n",
       " 'de',\n",
       " 'setembro',\n",
       " 'de',\n",
       " '1939',\n",
       " ',',\n",
       " 'foi',\n",
       " 'ordenado',\n",
       " 'sacerdote',\n",
       " 'em',\n",
       " 'nova',\n",
       " 'iorque',\n",
       " 'em',\n",
       " '2',\n",
       " 'de',\n",
       " 'julho',\n",
       " 'de',\n",
       " '1965',\n",
       " ',',\n",
       " 'pelo',\n",
       " 'mons',\n",
       " '.',\n",
       " 'aleksandar',\n",
       " 'tokić',\n",
       " ',',\n",
       " 'arcebispo',\n",
       " 'de',\n",
       " 'bar',\n",
       " ',',\n",
       " 'em',\n",
       " 'montenegro',\n",
       " ',',\n",
       " 'e',\n",
       " 'foi',\n",
       " 'um',\n",
       " 'padre',\n",
       " 'em',\n",
       " 'uma',\n",
       " 'paróquia',\n",
       " 'albanesa',\n",
       " 'no',\n",
       " 'bronx',\n",
       " ',',\n",
       " 'em',\n",
       " 'nova',\n",
       " 'iorque',\n",
       " '.',\n",
       " '\\n',\n",
       " 'em',\n",
       " '1º',\n",
       " 'de',\n",
       " 'julho',\n",
       " 'de',\n",
       " '1986',\n",
       " 'o',\n",
       " 'cardeal',\n",
       " 'john',\n",
       " 'joseph',\n",
       " 'o',\n",
       " \"'connor\",\n",
       " ',',\n",
       " 'arcebispo',\n",
       " 'de',\n",
       " 'nova',\n",
       " 'iorque',\n",
       " ',',\n",
       " 'nomeou',\n",
       " '-',\n",
       " 'o',\n",
       " 'chefe',\n",
       " 'da',\n",
       " 'igreja',\n",
       " 'católica',\n",
       " 'albanesa',\n",
       " '.',\n",
       " 'mirdita',\n",
       " 'foi',\n",
       " 'nomeado',\n",
       " 'arcebispo',\n",
       " 'da',\n",
       " 'tirana',\n",
       " '-',\n",
       " 'durrës',\n",
       " 'em',\n",
       " '25',\n",
       " 'de',\n",
       " 'dezembro',\n",
       " 'de',\n",
       " '1992',\n",
       " ',',\n",
       " 'e',\n",
       " 'consagrado',\n",
       " 'pelo',\n",
       " 'papa',\n",
       " 'joão',\n",
       " 'paulo',\n",
       " 'ii',\n",
       " 'em',\n",
       " 'pessoa',\n",
       " ',',\n",
       " 'durante',\n",
       " 'sua',\n",
       " 'visita',\n",
       " 'pastoral',\n",
       " 'à',\n",
       " 'albânia',\n",
       " 'em',\n",
       " '25',\n",
       " 'de',\n",
       " 'abril',\n",
       " 'de',\n",
       " '1993',\n",
       " ',',\n",
       " 'depois',\n",
       " 'de',\n",
       " 'a',\n",
       " 'arquidiocese',\n",
       " 'ficar',\n",
       " 'em',\n",
       " 'sede',\n",
       " 'vacante',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'de',\n",
       " 'quarenta',\n",
       " 'anos',\n",
       " '.',\n",
       " 'com',\n",
       " 'ele',\n",
       " ',',\n",
       " 'o',\n",
       " 'papa',\n",
       " 'consagrou',\n",
       " 'outros',\n",
       " 'três',\n",
       " 'bispos',\n",
       " 'zef',\n",
       " 'simoni',\n",
       " ',',\n",
       " 'frano',\n",
       " 'illia',\n",
       " 'e',\n",
       " 'robert',\n",
       " 'ashta',\n",
       " '.',\n",
       " 'desta',\n",
       " 'forma',\n",
       " ',',\n",
       " 'a',\n",
       " 'hierarquia',\n",
       " 'católica',\n",
       " 'foi',\n",
       " 'restabelecida',\n",
       " 'neste',\n",
       " 'país',\n",
       " ',',\n",
       " 'após',\n",
       " 'muitos',\n",
       " 'anos',\n",
       " 'de',\n",
       " 'perseguição',\n",
       " 'comunista',\n",
       " '.',\n",
       " '\\n',\n",
       " 'ele',\n",
       " 'é',\n",
       " 'o',\n",
       " 'presidente',\n",
       " 'da',\n",
       " 'conferência',\n",
       " 'episcopal',\n",
       " 'da',\n",
       " 'albânia',\n",
       " ',',\n",
       " 'bem',\n",
       " 'como',\n",
       " 'o',\n",
       " 'diretor',\n",
       " 'da',\n",
       " 'caritas',\n",
       " 'albanesa',\n",
       " '.',\n",
       " '\\n',\n",
       " 'por',\n",
       " 'sua',\n",
       " 'iniciativa',\n",
       " ',',\n",
       " 'foi',\n",
       " 'construída',\n",
       " 'a',\n",
       " 'catedral',\n",
       " 'de',\n",
       " 'são',\n",
       " 'paulo',\n",
       " 'em',\n",
       " 'tirana',\n",
       " '.',\n",
       " 'a',\n",
       " 'arquitetura',\n",
       " 'triangular',\n",
       " 'do',\n",
       " 'edifício',\n",
       " ',',\n",
       " 'de',\n",
       " 'acordo',\n",
       " 'com',\n",
       " 'o',\n",
       " 'projeto',\n",
       " 'do',\n",
       " 'arcebispo',\n",
       " ',',\n",
       " 'simboliza',\n",
       " 'a',\n",
       " 'coexistência',\n",
       " 'do',\n",
       " 'islamismo',\n",
       " ',',\n",
       " 'catolicismo',\n",
       " 'e',\n",
       " 'da',\n",
       " 'igreja',\n",
       " 'ortodoxa',\n",
       " 'na',\n",
       " 'albânia',\n",
       " '.',\n",
       " 'a',\n",
       " 'primeira',\n",
       " 'missa',\n",
       " 'na',\n",
       " 'nova',\n",
       " 'catedral',\n",
       " 'foi',\n",
       " 'celebrada',\n",
       " 'em',\n",
       " '27',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " 'de',\n",
       " '2002',\n",
       " 'pelo',\n",
       " 'então',\n",
       " 'secretário',\n",
       " 'de',\n",
       " 'estado',\n",
       " 'do',\n",
       " 'vaticano',\n",
       " ',',\n",
       " 'cardeal',\n",
       " 'angelo',\n",
       " 'sodano',\n",
       " '.',\n",
       " '\\n',\n",
       " 'no',\n",
       " 'dia',\n",
       " 'de',\n",
       " 'natal',\n",
       " 'de',\n",
       " '1999',\n",
       " ',',\n",
       " 'o',\n",
       " 'mons',\n",
       " '.',\n",
       " 'mirdita',\n",
       " 'conheceu',\n",
       " 'o',\n",
       " 'arcebispo',\n",
       " 'ortodoxo',\n",
       " 'anastasios',\n",
       " 'da',\n",
       " 'albânia',\n",
       " '.',\n",
       " '\\n',\n",
       " 'catolicismo',\n",
       " 'na',\n",
       " 'albânia',\n",
       " '\\n',\n",
       " 'a',\n",
       " 'prova',\n",
       " 'de',\n",
       " 'velocidade',\n",
       " 'individual',\n",
       " 'masculino',\n",
       " 'do',\n",
       " 'esqui',\n",
       " 'cross',\n",
       " '-',\n",
       " 'country',\n",
       " 'nos',\n",
       " 'jogos',\n",
       " 'olímpicos',\n",
       " 'de',\n",
       " 'inverno',\n",
       " 'de',\n",
       " '2014',\n",
       " 'ocorreu',\n",
       " 'no',\n",
       " 'dia',\n",
       " '11',\n",
       " 'de',\n",
       " 'fevereiro',\n",
       " 'no',\n",
       " 'centro',\n",
       " 'de',\n",
       " 'esqui',\n",
       " 'cross',\n",
       " '-',\n",
       " 'country',\n",
       " 'e',\n",
       " 'biatlo',\n",
       " 'laura',\n",
       " ',',\n",
       " 'na',\n",
       " 'clareira',\n",
       " 'vermelha',\n",
       " 'em',\n",
       " 'sóchi',\n",
       " '.',\n",
       " '\\n',\n",
       " 'q',\n",
       " 'qualificado',\n",
       " 'para',\n",
       " 'a',\n",
       " 'próxima',\n",
       " 'fase',\n",
       " '\\n',\n",
       " 'll',\n",
       " 'perdedor',\n",
       " 'de',\n",
       " 'sorte',\n",
       " '\\n',\n",
       " 'pf',\n",
       " 'photo',\n",
       " 'finish',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '1',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '2',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '3',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '4',\n",
       " '\\n',\n",
       " 'quartas',\n",
       " 'de',\n",
       " 'final',\n",
       " '5',\n",
       " '\\n',\n",
       " 'semifinal',\n",
       " '1',\n",
       " '\\n',\n",
       " 'semifinal',\n",
       " '2',\n",
       " '\\n',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'em',\n",
       " 'coreano',\n",
       " '아이스크림',\n",
       " 'é',\n",
       " 'uma',\n",
       " 'canção',\n",
       " 'de',\n",
       " 'gênero',\n",
       " 'dance',\n",
       " '-',\n",
       " 'electronic',\n",
       " 'interpretada',\n",
       " 'pelo',\n",
       " 'girl',\n",
       " 'group',\n",
       " 'sul',\n",
       " '-',\n",
       " 'coreano',\n",
       " 'f',\n",
       " 'x',\n",
       " '.',\n",
       " 'a',\n",
       " 'canção',\n",
       " 'foi',\n",
       " 'incluída',\n",
       " 'como',\n",
       " 'faixa',\n",
       " 'em',\n",
       " 'seu',\n",
       " 'primeiro',\n",
       " 'ep',\n",
       " ',',\n",
       " 'nu',\n",
       " '_tk_up_',\n",
       " 'abo',\n",
       " '.',\n",
       " '\\n',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'é',\n",
       " 'uma',\n",
       " 'canção',\n",
       " 'urbana',\n",
       " ',',\n",
       " 'que',\n",
       " 'pertence',\n",
       " 'ao',\n",
       " 'gênero',\n",
       " 'musical',\n",
       " 'dance',\n",
       " '-',\n",
       " 'electronic',\n",
       " '.',\n",
       " 'a',\n",
       " 'canção',\n",
       " 'foi',\n",
       " 'composta',\n",
       " 'pelo',\n",
       " 'produtor',\n",
       " 'musical',\n",
       " 'hitchhiker',\n",
       " 'que',\n",
       " 'já',\n",
       " 'havia',\n",
       " 'trabalhado',\n",
       " 'com',\n",
       " 'a',\n",
       " 'sm',\n",
       " 'entertainment',\n",
       " 'em',\n",
       " 'vários',\n",
       " 'projetos',\n",
       " ',',\n",
       " 'como',\n",
       " 'um',\n",
       " 'dos',\n",
       " 'seus',\n",
       " 'produtores',\n",
       " '.',\n",
       " 'mais',\n",
       " 'tarde',\n",
       " ',',\n",
       " 'ele',\n",
       " 'compôs',\n",
       " 'e',\n",
       " 'arranjou',\n",
       " 'o',\n",
       " 'single',\n",
       " 'do',\n",
       " 'f',\n",
       " 'x',\n",
       " 'pinocchio',\n",
       " 'danger',\n",
       " 'e',\n",
       " 'a',\n",
       " 'canção',\n",
       " 'sweet',\n",
       " 'witches',\n",
       " 'de',\n",
       " 'seu',\n",
       " 'primeiro',\n",
       " 'álbum',\n",
       " 'de',\n",
       " 'estúdio',\n",
       " 'pinocchio',\n",
       " ',',\n",
       " 'e',\n",
       " 'a',\n",
       " 'música',\n",
       " 'zig',\n",
       " 'zag',\n",
       " 'de',\n",
       " 'seu',\n",
       " 'segundo',\n",
       " 'ep',\n",
       " 'electric',\n",
       " 'shock',\n",
       " '.',\n",
       " 'a',\n",
       " 'letra',\n",
       " 'foi',\n",
       " 'escrita',\n",
       " 'por',\n",
       " 'kim',\n",
       " 'bumin',\n",
       " 'que',\n",
       " 'é',\n",
       " 'conhecido',\n",
       " 'por',\n",
       " 'trabalhar',\n",
       " 'em',\n",
       " 'colaboração',\n",
       " 'com',\n",
       " 'hitchhiker',\n",
       " 'escrevendo',\n",
       " 'letras',\n",
       " 'de',\n",
       " 'quase',\n",
       " 'todas',\n",
       " 'as',\n",
       " 'músicas',\n",
       " 'produzidas',\n",
       " 'por',\n",
       " 'ele',\n",
       " '.',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'foi',\n",
       " 'incluído',\n",
       " 'como',\n",
       " 'a',\n",
       " 'terceira',\n",
       " 'faixa',\n",
       " 'no',\n",
       " 'primeiro',\n",
       " 'ep',\n",
       " 'do',\n",
       " 'grupo',\n",
       " 'nu',\n",
       " '_tk_up_',\n",
       " 'abo',\n",
       " ',',\n",
       " 'que',\n",
       " 'foi',\n",
       " 'lançado',\n",
       " 'online',\n",
       " 'e',\n",
       " 'offline',\n",
       " 'em',\n",
       " '3',\n",
       " 'de',\n",
       " 'maio',\n",
       " 'de',\n",
       " '2010',\n",
       " '.',\n",
       " '\\n',\n",
       " 'uma',\n",
       " 'versão',\n",
       " 'remix',\n",
       " 'de',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'foi',\n",
       " 'produzida',\n",
       " 'pelo',\n",
       " 'grupo',\n",
       " 'idiotape',\n",
       " '.',\n",
       " 'ela',\n",
       " 'foi',\n",
       " 'lançado',\n",
       " 'como',\n",
       " 'uma',\n",
       " 'faixa',\n",
       " 'no',\n",
       " 'ep',\n",
       " '10',\n",
       " 'cc',\n",
       " 'x',\n",
       " 'sm',\n",
       " 'seoul',\n",
       " 'melody',\n",
       " ',',\n",
       " 'um',\n",
       " 'projeto',\n",
       " 'de',\n",
       " 'colaboração',\n",
       " 'conjunta',\n",
       " 'entre',\n",
       " 'a',\n",
       " 'sm',\n",
       " 'entertainment',\n",
       " 'e',\n",
       " '10',\n",
       " 'corso',\n",
       " 'como',\n",
       " ',',\n",
       " 'lançado',\n",
       " 'em',\n",
       " '27',\n",
       " 'de',\n",
       " 'março',\n",
       " 'de',\n",
       " '2013',\n",
       " '.',\n",
       " 'o',\n",
       " 'ep',\n",
       " 'também',\n",
       " 'continha',\n",
       " 'remixes',\n",
       " 'de',\n",
       " 'canções',\n",
       " 'de',\n",
       " 'sucesso',\n",
       " 'dos',\n",
       " 'companheiros',\n",
       " 'de',\n",
       " 'gravadora',\n",
       " 'do',\n",
       " 'f',\n",
       " 'x',\n",
       " ',',\n",
       " 'incluindo',\n",
       " 'before',\n",
       " 'u',\n",
       " 'go',\n",
       " 'do',\n",
       " '_tk_up_',\n",
       " 'tvxq',\n",
       " ',',\n",
       " 'sexy',\n",
       " ',',\n",
       " 'free',\n",
       " 'single',\n",
       " 'do',\n",
       " 'super',\n",
       " 'junior',\n",
       " ',',\n",
       " 'trick',\n",
       " 'do',\n",
       " 'girls',\n",
       " \"'\",\n",
       " 'generation',\n",
       " 'e',\n",
       " 'hello',\n",
       " 'do',\n",
       " 'shinee',\n",
       " '.',\n",
       " '\\n',\n",
       " 'ana',\n",
       " 'tereza',\n",
       " 'basílio',\n",
       " 'nasceu',\n",
       " '19',\n",
       " 'de',\n",
       " 'outubro',\n",
       " 'de',\n",
       " '1967',\n",
       " 'é',\n",
       " 'uma',\n",
       " 'juíza',\n",
       " 'e',\n",
       " 'advogada',\n",
       " 'brasileira',\n",
       " '.',\n",
       " 'é',\n",
       " 'juíza',\n",
       " 'substituta',\n",
       " 'do',\n",
       " 'tribunal',\n",
       " 'regional',\n",
       " 'eleitoral',\n",
       " ',',\n",
       " 'no',\n",
       " 'rio',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " ',',\n",
       " 'nomeada',\n",
       " 'pelo',\n",
       " 'presidente',\n",
       " 'luiz',\n",
       " 'inácio',\n",
       " 'lula',\n",
       " 'da',\n",
       " 'silva',\n",
       " 'em',\n",
       " '2010',\n",
       " ',',\n",
       " 'e',\n",
       " 'em',\n",
       " '2013',\n",
       " 'pela',\n",
       " 'presidente',\n",
       " 'da',\n",
       " 'república',\n",
       " 'dilma',\n",
       " 'rousseff',\n",
       " '.',\n",
       " '\\n',\n",
       " 'ana',\n",
       " 'basílio',\n",
       " 'é',\n",
       " 'bacharel',\n",
       " 'em',\n",
       " 'direito',\n",
       " 'pela',\n",
       " 'universidade',\n",
       " 'cândido',\n",
       " 'mendes',\n",
       " ',',\n",
       " 'no',\n",
       " 'rio',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " 'e',\n",
       " 'pós',\n",
       " 'graduada',\n",
       " 'em',\n",
       " 'direito',\n",
       " 'norte',\n",
       " 'americano',\n",
       " 'pela',\n",
       " 'universidade',\n",
       " 'de',\n",
       " 'wisconsin',\n",
       " ',',\n",
       " 'em',\n",
       " 'wisconsin',\n",
       " '.',\n",
       " '\\n',\n",
       " 'em',\n",
       " '1990',\n",
       " ',',\n",
       " 'inciou',\n",
       " 'sua',\n",
       " 'carreira',\n",
       " 'como',\n",
       " 'advogada',\n",
       " ',',\n",
       " 'atuando',\n",
       " 'em',\n",
       " 'escritório',\n",
       " 'de',\n",
       " 'advocacia',\n",
       " 'no',\n",
       " 'brasil',\n",
       " '.',\n",
       " 'foi',\n",
       " 'sócia',\n",
       " 'internacional',\n",
       " 'do',\n",
       " 'baker',\n",
       " 'mckenzie',\n",
       " '.',\n",
       " 'de',\n",
       " '2006',\n",
       " 'a',\n",
       " '2008',\n",
       " 'foi',\n",
       " 'sócia',\n",
       " 'no',\n",
       " 'escritório',\n",
       " 'andrade',\n",
       " 'fichtner',\n",
       " 'advogados',\n",
       " 'e',\n",
       " 'em',\n",
       " '2009',\n",
       " 'abriu',\n",
       " 'com',\n",
       " 'mais',\n",
       " 'quatro',\n",
       " 'sócios',\n",
       " 'o',\n",
       " 'escritório',\n",
       " 'de',\n",
       " 'advocacia',\n",
       " 'basílio',\n",
       " 'advogados',\n",
       " '.',\n",
       " '\\n',\n",
       " 'entre',\n",
       " 'os',\n",
       " 'anos',\n",
       " 'de',\n",
       " '2011',\n",
       " 'a',\n",
       " '2013',\n",
       " ',',\n",
       " 'foi',\n",
       " 'diretora',\n",
       " 'da',\n",
       " 'escola',\n",
       " 'judiciária',\n",
       " 'eleitoral',\n",
       " ',',\n",
       " 'no',\n",
       " 'rio',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " ',',\n",
       " 'vinculada',\n",
       " 'à',\n",
       " 'presidência',\n",
       " 'do',\n",
       " 'tribunal',\n",
       " 'regional',\n",
       " 'eleitoral',\n",
       " '.',\n",
       " 'sendo',\n",
       " 'responsável',\n",
       " 'pelas',\n",
       " 'novas',\n",
       " 'diretrizes',\n",
       " 'da',\n",
       " 'instituição',\n",
       " 'de',\n",
       " 'formar',\n",
       " ',',\n",
       " 'atualizar',\n",
       " 'e',\n",
       " 'especializar',\n",
       " 'magistrados',\n",
       " ',',\n",
       " 'membros',\n",
       " 'do',\n",
       " 'ministério',\n",
       " 'público',\n",
       " 'eleitoral',\n",
       " ',',\n",
       " 'servidores',\n",
       " 'do',\n",
       " 'tribunal',\n",
       " 'regional',\n",
       " 'eleitoral',\n",
       " '_tk_up_',\n",
       " 'tre',\n",
       " 'do',\n",
       " 'rio',\n",
       " 'de',\n",
       " 'janeiro',\n",
       " '.',\n",
       " '\\n',\n",
       " 'em',\n",
       " '2013',\n",
       " ',',\n",
       " 'foi',\n",
       " 'eleita',\n",
       " 'pela',\n",
       " 'chambers',\n",
       " 'e',\n",
       " 'partners',\n",
       " 'como',\n",
       " 'uma',\n",
       " 'das',\n",
       " 'referências',\n",
       " 'em',\n",
       " 'arbitragem',\n",
       " 'na',\n",
       " 'américa',\n",
       " 'latina',\n",
       " ',',\n",
       " 'e',\n",
       " 'eleita',\n",
       " 'advogada',\n",
       " 'do',\n",
       " 'ano',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/usherwood/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z5XZwijIQ6x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "corp_str = TreebankWordDetokenizer().detokenize(corpus).replace(' .', '.')\n",
    "for i in range(10):\n",
    "    corp_str = corp_str.replace('. '+str(i), '.'+str(i))\n",
    "\n",
    "corp_list = [x for x in sent_tokenize(corp_str) if x != '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jPZzpTvIQ63",
    "outputId": "bb79e4e5-f2bc-40ce-c3c5-7c4923c2fa70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['status, gender, and consumer nationalism in south korea, columbia university press, _tk_up_ isbn 0 - 231 - 11616 - 0 \\n yusuf, shahid; evenett, simon j., wu, weiping.2001 facets of globalization international and local dimensions of development world bank publications, pp.226 227 _tk_up_ isbn 0 - 8213 - 4742 - x \\n no, chŏng - hyŏn 1993 public administration and the korean transformation concepts, policies, and value conflicts, kumarian press, _tk_up_ isbn 1 - 56549 - 022 - 3 \\n dong pode se referir a \\n dong moeda \\n dong etnia \\n dong divisão administrativa \\n língua dong \\n lago dongting, também chamado dong \\n rrok kola mirdita klezna , 28 de setembro de 1939 é um arcebispo albanês.',\n",
       " 'tendo nascido em 28 de setembro de 1939, foi ordenado sacerdote em nova iorque em 2 de julho de 1965, pelo mons.',\n",
       " 'aleksandar tokić, arcebispo de bar, em montenegro, e foi um padre em uma paróquia albanesa no bronx, em nova iorque.',\n",
       " \"em 1º de julho de 1986 o cardeal john joseph o 'connor, arcebispo de nova iorque, nomeou - o chefe da igreja católica albanesa.\",\n",
       " 'mirdita foi nomeado arcebispo da tirana - durrës em 25 de dezembro de 1992, e consagrado pelo papa joão paulo ii em pessoa, durante sua visita pastoral à albânia em 25 de abril de 1993, depois de a arquidiocese ficar em sede vacante por mais de quarenta anos.',\n",
       " 'com ele, o papa consagrou outros três bispos zef simoni, frano illia e robert ashta.',\n",
       " 'desta forma, a hierarquia católica foi restabelecida neste país, após muitos anos de perseguição comunista.',\n",
       " 'ele é o presidente da conferência episcopal da albânia, bem como o diretor da caritas albanesa.',\n",
       " 'por sua iniciativa, foi construída a catedral de são paulo em tirana.',\n",
       " 'a arquitetura triangular do edifício, de acordo com o projeto do arcebispo, simboliza a coexistência do islamismo, catolicismo e da igreja ortodoxa na albânia.',\n",
       " 'a primeira missa na nova catedral foi celebrada em 27 de janeiro de 2002 pelo então secretário de estado do vaticano, cardeal angelo sodano.',\n",
       " 'no dia de natal de 1999, o mons.',\n",
       " 'mirdita conheceu o arcebispo ortodoxo anastasios da albânia.',\n",
       " 'catolicismo na albânia \\n a prova de velocidade individual masculino do esqui cross - country nos jogos olímpicos de inverno de 2014 ocorreu no dia 11 de fevereiro no centro de esqui cross - country e biatlo laura, na clareira vermelha em sóchi.',\n",
       " 'q qualificado para a próxima fase \\n ll perdedor de sorte \\n pf photo finish \\n quartas de final 1 \\n quartas de final 2 \\n quartas de final 3 \\n quartas de final 4 \\n quartas de final 5 \\n semifinal 1 \\n semifinal 2 \\n ice cream em coreano 아이스크림 é uma canção de gênero dance - electronic interpretada pelo girl group sul - coreano f x. a canção foi incluída como faixa em seu primeiro ep, nu _tk_up_ abo.',\n",
       " 'ice cream é uma canção urbana, que pertence ao gênero musical dance - electronic.',\n",
       " 'a canção foi composta pelo produtor musical hitchhiker que já havia trabalhado com a sm entertainment em vários projetos, como um dos seus produtores.',\n",
       " 'mais tarde, ele compôs e arranjou o single do f x pinocchio danger e a canção sweet witches de seu primeiro álbum de estúdio pinocchio, e a música zig zag de seu segundo ep electric shock.',\n",
       " 'a letra foi escrita por kim bumin que é conhecido por trabalhar em colaboração com hitchhiker escrevendo letras de quase todas as músicas produzidas por ele.',\n",
       " 'ice cream foi incluído como a terceira faixa no primeiro ep do grupo nu _tk_up_ abo, que foi lançado online e offline em 3 de maio de 2010. \\n uma versão remix de ice cream foi produzida pelo grupo idiotape.',\n",
       " \"ela foi lançado como uma faixa no ep 10 cc x sm seoul melody, um projeto de colaboração conjunta entre a sm entertainment e 10 corso como, lançado em 27 de março de 2013. o ep também continha remixes de canções de sucesso dos companheiros de gravadora do f x, incluindo before u go do _tk_up_ tvxq, sexy, free single do super junior, trick do girls' generation e hello do shinee.\",\n",
       " 'ana tereza basílio nasceu 19 de outubro de 1967 é uma juíza e advogada brasileira.',\n",
       " 'é juíza substituta do tribunal regional eleitoral, no rio de janeiro, nomeada pelo presidente luiz inácio lula da silva em 2010, e em 2013 pela presidente da república dilma rousseff.',\n",
       " 'ana basílio é bacharel em direito pela universidade cândido mendes, no rio de janeiro e pós graduada em direito norte americano pela universidade de wisconsin, em wisconsin.',\n",
       " 'em 1990, inciou sua carreira como advogada, atuando em escritório de advocacia no brasil.',\n",
       " 'foi sócia internacional do baker mckenzie.',\n",
       " 'de 2006 a 2008 foi sócia no escritório andrade fichtner advogados e em 2009 abriu com mais quatro sócios o escritório de advocacia basílio advogados.',\n",
       " 'entre os anos de 2011 a 2013, foi diretora da escola judiciária eleitoral, no rio de janeiro, vinculada à presidência do tribunal regional eleitoral.',\n",
       " 'sendo responsável pelas novas diretrizes da instituição de formar, atualizar e especializar magistrados, membros do ministério público eleitoral, servidores do tribunal regional eleitoral _tk_up_ tre do rio de janeiro.',\n",
       " 'em 2013, foi eleita pela chambers e partners como uma das referências em arbitragem na américa latina, e eleita advogada do ano pela global awards 2012, nas áreas de contencioso civil e empresarial.',\n",
       " 'em 2012 foi nomeada presidente da comissão de direito eleitoral do instituto dos advogados do brasil \\n em 2010, o presidente da república luiz inácio lula da silva nomeou ana tereza basílio, ao cargo de juíza substituta do tribunal regional eleitoral do estado do rio de janeiro.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_list[:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYTeqNO9IQ68",
    "outputId": "d4f281a6-7479-4c6d-f31d-d85f09acff45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "em 2012 foi nomeada presidente da comissão de direito eleitoral do instituto dos advogados do brasil \n",
      " em 2010, o presidente da república luiz inácio lula da silva nomeou ana tereza basílio, ao cargo de juíza substituta do tribunal regional eleitoral do estado do rio de janeiro.\n",
      "['em', '2012', 'foi', 'nomeada', 'presidente', 'da', 'comissão', 'de', 'direito', 'eleitoral', 'do', 'instituto', 'dos', 'advogados', 'do', 'brasil', 'em', '2010', ',', 'o', 'presidente', 'da', 'república', 'luiz', 'inácio', 'lula', 'da', 'silva', 'nomeou', 'ana', 'tere', 'za', 'basílio', ',', 'ao', 'cargo', 'de', 'juí', 'za', 'substitu', 'ta', 'do', 'tribunal', 'regional', 'eleitoral', 'do', 'estado', 'do', 'rio', 'de', 'janeiro', '.']\n"
     ]
    }
   ],
   "source": [
    "print(corp_list[30])\n",
    "print(bert_tokenizer.tokenize(corp_list[30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7WGWr8RJIQ7A"
   },
   "outputs": [],
   "source": [
    "TRAIN_VAL_SPLIT = .1\n",
    "train_size = int(len(corp_list)*TRAIN_VAL_SPLIT)\n",
    "\n",
    "df_train = pd.DataFrame(corp_list[:train_size])\n",
    "df_val = pd.DataFrame(corp_list[train_size:])\n",
    "df_val.to_csv(processed_data_dir / \"validate\" / \"val.csv\", index=False, header=None, quoting=csv.QUOTE_MINIMAL, encoding='utf-8')\n",
    "\n",
    "step = int(train_size/100)\n",
    "for i in range(100):\n",
    "    df_train[step*i:step*(i+1)].to_csv(processed_data_dir / \"train\" / \"train{}.csv\".format(i), index=False, header=None, quoting=csv.QUOTE_MINIMAL, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Prep Data and Create Tokenizer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
