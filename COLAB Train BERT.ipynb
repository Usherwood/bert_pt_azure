{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ST3bVHvDdv8J"
   },
   "source": [
    "# Colab Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56879,
     "status": "ok",
     "timestamp": 1564172060749,
     "user": {
      "displayName": "Peter Usherwood",
      "photoUrl": "https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg",
      "userId": "11459160037307263961"
     },
     "user_tz": 180
    },
    "id": "AOzhl1jgWaAl",
    "outputId": "c8393443-efcd-47a8-d345-3d99e2255fe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "路路路路路路路路路路\n",
      "Mounted at /content/drive\n",
      "Cloning into 'bert_pt'...\n",
      "remote: Enumerating objects: 102, done.\u001b[K\n",
      "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
      "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
      "remote: Total 102 (delta 52), reused 78 (delta 31), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (102/102), 130.60 KiB | 3.35 MiB/s, done.\n",
      "Resolving deltas: 100% (52/52), done.\n",
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
      "\u001b[K     || 655kB 3.4MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: regex\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
      "Successfully built regex\n",
      "Installing collected packages: regex\n",
      "Successfully installed regex-2019.6.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 20:14:19.927634 140038492882816 deprecation_wrapper.py:119] From /content/bert_pt/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "COLAB = True\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!git clone https://Usherwood:DN6a1q9f@github.com/Usherwood/bert_pt.git\n",
    "    \n",
    "!pip install regex\n",
    "\n",
    "from bert_pt.bert import modeling, optimization, tokenization\n",
    "from bert_pt.bert.run_pretraining import input_fn_builder, model_fn_builder\n",
    "\n",
    "from bert_pt.bert.text_preprocessing import tokenizer_word\n",
    "from bert_pt.bert.language_model_processing import read_raw_data_preprocess_and_save, create_vocab_df\n",
    "from bert_pt.bert.bpe import create_token_vocabulary, get_stats, merge_vocab, Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vpj3WvvCWsBG"
   },
   "source": [
    "# Local Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pnPU5O9Ss-6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "import pickle\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6ltt0dpSs_B"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'wiki_pt'\n",
    "INPUT_TYPE = 'txt' #Options: tokens, txt, csv\n",
    "TO_SPLIT_CLITICS = True #Set to false if clitics already tokenized\n",
    "DATASET_FILE_MAP = {'all': 'pt_wiki_corpus_v1_0_1.txt'}\n",
    "\n",
    "if DATASET_FILE_MAP.get('all'):\n",
    "    file_split = 'all'\n",
    "else:\n",
    "    file_split = 'split'\n",
    "\n",
    "UNK_TOKEN = None #none if isnt one\n",
    "SEQUENCE_LENGTH = 20\n",
    "NUM_MERGES = 10000 #VOCABULARY_SIZE = NUM_MERGES + N_BYTES (~1500)\n",
    "\n",
    "mini_batch_size = 64\n",
    "N_SPLITS = 10 #of training data on disk, this is purely a question of resource efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jacXVd3aSs_E"
   },
   "outputs": [],
   "source": [
    "notebook_dir = pathlib.Path.cwd()\n",
    "repo_dir = notebook_dir.parent\n",
    "if COLAB:\n",
    "  repo_dir = repo_dir / 'content' / 'drive' / 'My Drive' / 'Data Science' / 'bert_pt'\n",
    "(repo_dir / \"models\").mkdir(exist_ok = True)\n",
    "(repo_dir / \"models\" / \"base\").mkdir(exist_ok = True)\n",
    "dataset_dir = repo_dir / \"datasets\" / \"base\" / DATASET_NAME\n",
    "models_dir = repo_dir / \"models\" / \"base\"\n",
    "(models_dir / DATASET_NAME).mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"preprocessed_base_data\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\" / \"train\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"processed_base_data\" / \"validate\").mkdir(exist_ok = True)\n",
    "(models_dir / DATASET_NAME / \"pretraining_base_data\").mkdir(exist_ok = True)\n",
    "processed_data_dir = models_dir / DATASET_NAME / \"processed_base_data\"\n",
    "pretraining_data_dir = models_dir / DATASET_NAME / \"pretraining_base_data\"\n",
    "weights_dir = models_dir / DATASET_NAME / \"model_checkpoints\"\n",
    "(models_dir / DATASET_NAME / \"language_maps\").mkdir(exist_ok = True)\n",
    "language_maps_dir = repo_dir / \"models\" / \"base\" / DATASET_NAME / \"language_maps\"\n",
    "(language_maps_dir).mkdir(exist_ok = True)\n",
    "    \n",
    "models_dir = models_dir / DATASET_NAME\n",
    "\n",
    "def save_obj(obj, directory, name):\n",
    "    with open(directory / \"{}.pkl\".format(name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name, directory):\n",
    "    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "      \n",
    "      \n",
    "vocab_to_id = load_obj('vocab_to_id', str(language_maps_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25225,
     "status": "ok",
     "timestamp": 1564172064540,
     "user": {
      "displayName": "Peter Usherwood",
      "photoUrl": "https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg",
      "userId": "11459160037307263961"
     },
     "user_tz": 180
    },
    "id": "UjS7A4BSSs_W",
    "outputId": "3ed09f9d-8722-4404-e3be-995c1e6e8bf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ol谩 isso 茅 mais uma BAGUNCA \n",
      "['ol', '谩', 'isso', '茅', 'mais', 'uma', 'ba', 'gun', 'ca', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "testcase = \"Ol谩 isso 茅 mais uma BAGUNCA \"\n",
    "bert_tokenizer = tokenization.FullTokenizer(language_maps_dir)\n",
    "print(testcase)\n",
    "print(bert_tokenizer.tokenize(testcase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1564172772006,
     "user": {
      "displayName": "Peter Usherwood",
      "photoUrl": "https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg",
      "userId": "11459160037307263961"
     },
     "user_tz": 180
    },
    "id": "SstzQ1fNMjC1",
    "outputId": "20d124e4-3b68-48dc-a0c3-cdc3eeb0a385"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11503"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hl5lIHnbSs_w"
   },
   "source": [
    "# Train BERT\n",
    "run this on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvC9nv9x85QH"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"bert_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
    "PRETRAINING_DIR = pretraining_data_dir.absolute() #@param {type:\"string\"}\n",
    "tf.gfile.MkDir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7155,
     "status": "ok",
     "timestamp": 1564173165921,
     "user": {
      "displayName": "Peter Usherwood",
      "photoUrl": "https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg",
      "userId": "11459160037307263961"
     },
     "user_tz": 180
    },
    "id": "8NwPqum3Ss_y",
    "outputId": "c9dfe48d-4754-47e7-9548-7135abe90b04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: No URLs matched: /content/drive/My\n",
      "CommandException: No URLs matched: Drive/Data\n",
      "CommandException: No URLs matched: Science/bert_pt/models/base/wiki_pt/pretraining_base_data\n",
      "Copying file://bert_model/bert_config.json [Content-Type=application/json]...\n",
      "/ [0/2 files][    0.0 B/ 72.4 KiB]   0% Done                                    \r",
      "Copying file://bert_model/vocab_file.csv [Content-Type=text/csv]...\n",
      "/ [0/2 files][    0.0 B/ 72.4 KiB]   0% Done                                    \r",
      "ServiceException: 401 Anonymous caller does not have storage.objects.create access to bert_resourses/bert_model/vocab_file.csv.\n",
      "ServiceException: 401 Anonymous caller does not have storage.objects.create access to bert_resourses/bert_model/bert_config.json.\n",
      "CommandException: 5 files/objects could not be transferred.\n"
     ]
    }
   ],
   "source": [
    "# use this for BERT-base\n",
    "\n",
    "bert_base_config = {\n",
    "  \"attention_probs_dropout_prob\": 0.1, \n",
    "  \"directionality\": \"bidi\", \n",
    "  \"hidden_act\": \"gelu\", \n",
    "  \"hidden_dropout_prob\": 0.1, \n",
    "  \"hidden_size\": 768, \n",
    "  \"initializer_range\": 0.02, \n",
    "  \"intermediate_size\": 3072, \n",
    "  \"max_position_embeddings\": 512, \n",
    "  \"num_attention_heads\": 12, \n",
    "  \"num_hidden_layers\": 12, \n",
    "  \"pooler_fc_size\": 768, \n",
    "  \"pooler_num_attention_heads\": 12, \n",
    "  \"pooler_num_fc_layers\": 3, \n",
    "  \"pooler_size_per_head\": 128, \n",
    "  \"pooler_type\": \"first_token_transform\", \n",
    "  \"type_vocab_size\": 2, \n",
    "  \"vocab_size\": len(vocab_to_id)\n",
    "}\n",
    "\n",
    "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
    "  json.dump(bert_base_config, fo, indent=2)\n",
    "  \n",
    "  \n",
    "with open(\"{}/{}\".format(MODEL_DIR, \"vocab_file.csv\"), \"w\") as fo:\n",
    "  for token in list(vocab_to_id.keys()):\n",
    "    fo.write(token+\"\\n\")\n",
    "    \n",
    "\n",
    "if BUCKET_NAME:\n",
    "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZS6UsSAmea1"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"bert_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
    "VOC_FNAME = \"vocab_file.csv\" #@param {type:\"string\"}\n",
    "\n",
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
    "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "\n",
    "BERT_GCS_DIR = str(weights_dir)#.replace(r' ',r'\\ ')\n",
    "DATA_GCS_DIR = str(pretraining_data_dir)#.replace(r' ',r'\\ ')\n",
    "\n",
    "VOCAB_FILE = str(language_maps_dir / 'vocab_file.csv')#.replace(r' ',r'\\ ')\n",
    "CONFIG_FILE = str(models_dir / \"bert_config.json\")#.replace(r' ',r'\\ ')\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
    "\n",
    "USE_TPU = True\n",
    "\n",
    "if not USE_TPU:\n",
    "    import tensorflow as tf\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "      raise SystemError('GPU device not found')\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "    session_target = ''\n",
    "else:\n",
    "    import os\n",
    "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "    session_target = tpu_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1332,
     "status": "ok",
     "timestamp": 1564172128230,
     "user": {
      "displayName": "Peter Usherwood",
      "photoUrl": "https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg",
      "userId": "11459160037307263961"
     },
     "user_tz": 180
    },
    "id": "vbtSC-8PnUVb",
    "outputId": "0e048524-22e1-4966-d7bc-420afd4bdb56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=/content\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH=/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5221,
     "status": "ok",
     "timestamp": 1564172132824,
     "user": {
      "displayName": "Peter Usherwood",
      "photoUrl": "https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg",
      "userId": "11459160037307263961"
     },
     "user_tz": 180
    },
    "id": "qgJJuxvmmV5l",
    "outputId": "98bd1da7-91a3-4b92-cf83-e96ffc1b61d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /content: Is a directory\n"
     ]
    }
   ],
   "source": [
    "!$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2399,
     "status": "ok",
     "timestamp": 1564172135727,
     "user": {
      "displayName": "Peter Usherwood",
      "photoUrl": "https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg",
      "userId": "11459160037307263961"
     },
     "user_tz": 180
    },
    "id": "qa-SA1qbSs_2",
    "outputId": "f5d02546-4672-4852-f05e-1c196c477cd3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 20:15:34.835923 140038492882816 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f5cf8f5e2f0>) includes params argument, but params are not passed to Estimator.\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "\n",
    "run_config = tf.estimaotr.RunConfig(\n",
    "    model_dir=model_weigths_dir,\n",
    "    tf_random_seed=None,\n",
    "    save_summary_steps=100,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    session_config=None,\n",
    "    keep_checkpoint_max=5,\n",
    "    keep_checkpoint_every_n_hours=24,\n",
    "    log_step_count_steps=100,\n",
    "    train_distribute=None,\n",
    "    device_fn=None,\n",
    "    protocol=None,\n",
    "    eval_distribute=None,\n",
    "    experimental_distribute=None,\n",
    "    experimental_max_worker_delay_secs=None)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  \n",
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45205,
     "status": "error",
     "timestamp": 1564172183574,
     "user": {
      "displayName": "Peter Usherwood",
      "photoUrl": "https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg",
      "userId": "11459160037307263961"
     },
     "user_tz": 180
    },
    "id": "KPjDIYtRfpph",
    "outputId": "dd9a6102-2dfe-40db-d51d-1477703ceb4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 20:15:38.826808 140038492882816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0726 20:15:38.857530 140038492882816 deprecation_wrapper.py:119] From /content/bert_pt/bert/run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "W0726 20:15:38.871272 140038492882816 deprecation.py:323] From /content/bert_pt/bert/run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.parallel_interleave(...)`.\n",
      "W0726 20:15:38.872646 140038492882816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W0726 20:15:38.927081 140038492882816 deprecation.py:323] From /content/bert_pt/bert/run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "W0726 20:15:38.928693 140038492882816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "W0726 20:15:38.933088 140038492882816 deprecation_wrapper.py:119] From /content/bert_pt/bert/run_pretraining.py:393: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "W0726 20:15:38.953576 140038492882816 deprecation.py:323] From /content/bert_pt/bert/run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0726 20:15:39.057776 140038492882816 deprecation_wrapper.py:119] From /content/bert_pt/bert/run_pretraining.py:117: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W0726 20:15:39.060077 140038492882816 deprecation_wrapper.py:119] From /content/bert_pt/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0726 20:15:39.066619 140038492882816 deprecation_wrapper.py:119] From /content/bert_pt/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0726 20:15:39.117888 140038492882816 deprecation_wrapper.py:119] From /content/bert_pt/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "W0726 20:15:39.186624 140038492882816 deprecation.py:506] From /content/bert_pt/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0726 20:15:39.219156 140038492882816 deprecation.py:323] From /content/bert_pt/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0726 20:15:43.892250 140038492882816 deprecation_wrapper.py:119] From /content/bert_pt/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "W0726 20:15:43.895223 140038492882816 deprecation_wrapper.py:119] From /content/bert_pt/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n",
      "W0726 20:15:43.908656 140038492882816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0726 20:15:44.272941 140038492882816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "E0726 20:16:22.575618 140038492882816 error_handling.py:70] Error recorded from training_loop: From /job:worker/replica:0/task:0:\n",
      "File system scheme '[local]' not implemented (file: '/content/drive/My Drive/Data Science/bert_pt/models/base/wiki_pt/model_checkpoints/model.ckpt-0_temp_5453d7707d4c4c3a9843264176bfc15f')\n",
      "\t [[node save/SaveV2 (defined at <ipython-input-15-fc1a0ca3c649>:1) ]]\n",
      "\n",
      "Errors may have originated from an input operation.\n",
      "Input Source operations connected to node save/SaveV2:\n",
      " bert/encoder/layer_6/intermediate/dense/kernel/adam_v/Read/ReadVariableOp (defined at /content/bert_pt/bert/optimization.py:128)\t\n",
      " bert/encoder/layer_7/attention/self/query/bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:671)\t\n",
      " bert/encoder/layer_2/output/dense/bias/adam_m/Read/ReadVariableOp (defined at /content/bert_pt/bert/optimization.py:122)\t\n",
      " bert/encoder/layer_4/attention/output/dense/bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:861)\t\n",
      " bert/encoder/layer_9/output/dense/bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:878)\t\n",
      " bert/encoder/layer_9/attention/self/value/bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:687)\t\n",
      " bert/encoder/layer_9/attention/self/key/kernel/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:679)\t\n",
      " cls/seq_relationship/output_weights/Read/ReadVariableOp (defined at /content/bert_pt/bert/run_pretraining.py:294)\t\n",
      " bert/encoder/layer_4/attention/output/LayerNorm/gamma/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:365)\t\n",
      " cls/seq_relationship/output_bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/run_pretraining.py:296)\t\n",
      " bert/encoder/layer_11/intermediate/dense/kernel/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:871)\t\n",
      " bert/pooler/dense/kernel/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:232)\t\n",
      " cls/predictions/transform/dense/kernel/Read/ReadVariableOp (defined at /content/bert_pt/bert/run_pretraining.py:254)\t\n",
      " bert/embeddings/position_embeddings/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:495)\t\n",
      " bert/embeddings/token_type_embeddings/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:479)\t\n",
      " cls/predictions/output_bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/run_pretraining.py:262)\t\n",
      " bert/embeddings/word_embeddings/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:412)\n",
      "\n",
      "Original stack trace for 'save/SaveV2':\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-fc1a0ca3c649>\", line 1, in <module>\n",
      "    estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 2871, in train\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\n",
      "    saving_listeners)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1480, in _train_with_estimator_spec\n",
      "    log_step_count_steps=log_step_count_steps) as mon_sess:\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 584, in MonitoredTrainingSession\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1007, in __init__\n",
      "    stop_grace_period_secs=stop_grace_period_secs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 725, in __init__\n",
      "    self._sess = _RecoverableSession(self._coordinated_creator)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1200, in __init__\n",
      "    _WrappedSession.__init__(self, self._create_session())\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1205, in _create_session\n",
      "    return self._sess_creator.create_session()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 871, in create_session\n",
      "    self.tf_sess = self._session_creator.create_session()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 638, in create_session\n",
      "    self._scaffold.finalize()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 237, in finalize\n",
      "    self._saver.build()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 837, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 875, in _build\n",
      "    build_restore=build_restore)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 499, in _build_internal\n",
      "    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 291, in _AddShardedSaveOps\n",
      "    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 265, in _AddShardedSaveOpsForV2\n",
      "    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 206, in _AddSaveOps\n",
      "    save = self.save_op(filename_tensor, saveables)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 122, in save_op\n",
      "    tensors)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1946, in save_v2\n",
      "    name=name)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "W0726 20:16:22.577968 140038492882816 error_handling.py:130] Reraising captured error\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: From /job:worker/replica:0/task:0:\nFile system scheme '[local]' not implemented (file: '/content/drive/My Drive/Data Science/bert_pt/models/base/wiki_pt/model_checkpoints/model.ckpt-0_temp_5453d7707d4c4c3a9843264176bfc15f')\n\t [[{{node save/SaveV2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fc1a0ca3c649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2875\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2876\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   2869\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2871\u001b[0;31m           saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m   2872\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2873\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1190\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1191\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0msave_summaries_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_summary_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m         log_step_count_steps=log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[1;32m   1481\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[0;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\u001b[0m\n\u001b[1;32m    582\u001b[0m       \u001b[0msession_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m       stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    723\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \"\"\"\n\u001b[1;32m   1199\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         logging.info(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# Inform the hooks that a new session has been created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m       return _CoordinatedSession(\n\u001b[1;32m    880\u001b[0m           \u001b[0m_HookedSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36mafter_create_session\u001b[0;34m(self, session, coord)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;31m# The checkpoint saved here is the state at step \"global_step\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_last_triggered_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self, session, step)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     self._summary_writer.add_session_log(\n\u001b[1;32m    613\u001b[0m         SessionLog(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1171\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1172\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: From /job:worker/replica:0/task:0:\nFile system scheme '[local]' not implemented (file: '/content/drive/My Drive/Data Science/bert_pt/models/base/wiki_pt/model_checkpoints/model.ckpt-0_temp_5453d7707d4c4c3a9843264176bfc15f')\n\t [[node save/SaveV2 (defined at <ipython-input-15-fc1a0ca3c649>:1) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node save/SaveV2:\n bert/encoder/layer_6/intermediate/dense/kernel/adam_v/Read/ReadVariableOp (defined at /content/bert_pt/bert/optimization.py:128)\t\n bert/encoder/layer_7/attention/self/query/bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:671)\t\n bert/encoder/layer_2/output/dense/bias/adam_m/Read/ReadVariableOp (defined at /content/bert_pt/bert/optimization.py:122)\t\n bert/encoder/layer_4/attention/output/dense/bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:861)\t\n bert/encoder/layer_9/output/dense/bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:878)\t\n bert/encoder/layer_9/attention/self/value/bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:687)\t\n bert/encoder/layer_9/attention/self/key/kernel/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:679)\t\n cls/seq_relationship/output_weights/Read/ReadVariableOp (defined at /content/bert_pt/bert/run_pretraining.py:294)\t\n bert/encoder/layer_4/attention/output/LayerNorm/gamma/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:365)\t\n cls/seq_relationship/output_bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/run_pretraining.py:296)\t\n bert/encoder/layer_11/intermediate/dense/kernel/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:871)\t\n bert/pooler/dense/kernel/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:232)\t\n cls/predictions/transform/dense/kernel/Read/ReadVariableOp (defined at /content/bert_pt/bert/run_pretraining.py:254)\t\n bert/embeddings/position_embeddings/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:495)\t\n bert/embeddings/token_type_embeddings/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:479)\t\n cls/predictions/output_bias/Read/ReadVariableOp (defined at /content/bert_pt/bert/run_pretraining.py:262)\t\n bert/embeddings/word_embeddings/Read/ReadVariableOp (defined at /content/bert_pt/bert/modeling.py:412)\n\nOriginal stack trace for 'save/SaveV2':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-fc1a0ca3c649>\", line 1, in <module>\n    estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 2871, in train\n    saving_listeners=saving_listeners)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\n    return self._train_model_default(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\n    saving_listeners)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1480, in _train_with_estimator_spec\n    log_step_count_steps=log_step_count_steps) as mon_sess:\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 584, in MonitoredTrainingSession\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1007, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 725, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1200, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1205, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 871, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 638, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 237, in finalize\n    self._saver.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 837, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 875, in _build\n    build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 499, in _build_internal\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 291, in _AddShardedSaveOps\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 265, in _AddShardedSaveOpsForV2\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 206, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 122, in save_op\n    tensors)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1946, in save_v2\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cw2vEfFQSs_7"
   },
   "source": [
    "# Dev work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1X6XE-VVSs_8"
   },
   "outputs": [],
   "source": [
    "byte_decoder = load_obj(\"byte_decoder\", language_maps_dir)\n",
    "id_to_vocab = load_obj(\"id_to_vocab\", language_maps_dir)\n",
    "vocab_to_id = load_obj(\"vocab_to_id\", language_maps_dir)\n",
    "bpe_merges = load_obj(\"bpe_merges\", language_maps_dir)\n",
    "\n",
    "encoder = Encoder(\n",
    "                  encoder=vocab_to_id,\n",
    "                  bpe_merges=bpe_merges,\n",
    "                  byte_decoder=byte_decoder,\n",
    "                  unk_token='[UNK]',\n",
    "                  unk_id=len(byte_decoder)-6,\n",
    "                  spc_token=chr(int(\"E001\", 16)),\n",
    "                  spc_id=len(byte_decoder)-5)\n",
    "\n",
    "testcase = \" \".join(words[1000:1020])\n",
    "print(testcase)\n",
    "print(encoder.tokenize(testcase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Azi5iTpJSs_-"
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://configandvocab@ktbrdsdevstorage.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/configandvocab\",\n",
    "  extra_configs = {\"fs.azure.account.key.ktbrdsdevstorage.blob.core.windows.net\":dbutils.secrets.get(scope = \"bert_pt_databricks_scope\", key = \"bertptkey\")})\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://modelweights@ktbrdsdevstorage.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/modelweights\",\n",
    "  extra_configs = {\"fs.azure.account.key.ktbrdsdevstorage.blob.core.windows.net\":dbutils.secrets.get(scope = \"bert_pt_databricks_scope\", key = \"bertptkey\")})\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://pretrainingbasedata@ktbrdsdevstorage.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/pretrainingbasedata\",\n",
    "  extra_configs = {\"fs.azure.account.key.ktbrdsdevstorage.blob.core.windows.net\":dbutils.secrets.get(scope = \"bert_pt_databricks_scope\", key = \"bertptkey\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "geE5xDtBStAC"
   },
   "outputs": [],
   "source": [
    "%sh ls /dbfs/mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_obj(name, directory):\n",
    "    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "      \n",
    "      \n",
    "vocab_to_id = load_obj('vocab_to_id', '/dbfs/mnt/configandvocab')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "COLAB Train BERT.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
