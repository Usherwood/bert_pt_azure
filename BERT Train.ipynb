{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">configandvocab\n",
       "modelweights\n",
       "pretrainingbasedata\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh ls /dbfs/mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount drives\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://configandvocab@ktbrdsdevstorage.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/configandvocab\",\n",
    "  extra_configs = {\"fs.azure.account.key.ktbrdsdevstorage.blob.core.windows.net\":dbutils.secrets.get(scope = \"bert_pt_databricks_scope\", key = \"bertptkey\")})\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://modelweights@ktbrdsdevstorage.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/modelweights\",\n",
    "  extra_configs = {\"fs.azure.account.key.ktbrdsdevstorage.blob.core.windows.net\":dbutils.secrets.get(scope = \"bert_pt_databricks_scope\", key = \"bertptkey\")})\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://pretrainingbasedata@ktbrdsdevstorage.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/pretrainingbasedata\",\n",
    "  extra_configs = {\"fs.azure.account.key.ktbrdsdevstorage.blob.core.windows.net\":dbutils.secrets.get(scope = \"bert_pt_databricks_scope\", key = \"bertptkey\")})\n",
    "\n",
    "%sh ls /dbfs/mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Cloning into &#39;bert_pt_azure&#39;...\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh git clone https://Usherwood:DN6a1q9f@github.com/Usherwood/bert_pt_azure.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">total 164K\n",
       "drwxr-xr-x 4 root root 4.0K Aug  9 19:40 .\n",
       "drwxr-xr-x 1 root root 4.0K Aug  9 19:40 ..\n",
       "drwxr-xr-x 2 root root 4.0K Aug  9 19:40 bert\n",
       "-rw-r--r-- 1 root root  15K Aug  9 19:40 COLAB Create Training Data.ipynb\n",
       "-rw-r--r-- 1 root root  77K Aug  9 19:40 COLAB Train BERT.ipynb\n",
       "drwxr-xr-x 8 root root 4.0K Aug  9 19:40 .git\n",
       "-rw-r--r-- 1 root root 1.4K Aug  9 19:40 .gitignore\n",
       "-rw-r--r-- 1 root root    0 Aug  9 19:40 __init__.py\n",
       "-rw-r--r-- 1 root root  45K Aug  9 19:40 Prep Data and Create Tokenizer.ipynb\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh ls -alh /databricks/driver/bert_pt_azure/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to system, local or mounted S3 bucket, e.g. /dbfs/mnt/<path_to_bucket>\n",
    "sys.path.append('/databricks/driver/bert_pt_azure/bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Python 2.7.15 :: Anaconda, Inc.\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">ImportError</span>                               Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span><span class=\"ansi-blue-fg\">()</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     57</span> \n",
       "<span class=\"ansi-green-fg\">---&gt; 58</span><span class=\"ansi-red-fg\">   </span><span class=\"ansi-green-fg\">from</span> tensorflow<span class=\"ansi-blue-fg\">.</span>python<span class=\"ansi-blue-fg\">.</span>pywrap_tensorflow_internal <span class=\"ansi-green-fg\">import</span> <span class=\"ansi-blue-fg\">*</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     59</span>   <span class=\"ansi-green-fg\">from</span> tensorflow<span class=\"ansi-blue-fg\">.</span>python<span class=\"ansi-blue-fg\">.</span>pywrap_tensorflow_internal <span class=\"ansi-green-fg\">import</span> __version__\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span><span class=\"ansi-blue-fg\">()</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     27</span>             <span class=\"ansi-green-fg\">return</span> _mod\n",
       "<span class=\"ansi-green-fg\">---&gt; 28</span><span class=\"ansi-red-fg\">     </span>_pywrap_tensorflow_internal <span class=\"ansi-blue-fg\">=</span> swig_import_helper<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>     <span class=\"ansi-green-fg\">del</span> swig_import_helper\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py</span> in <span class=\"ansi-cyan-fg\">swig_import_helper</span><span class=\"ansi-blue-fg\">()</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     23</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 24</span><span class=\"ansi-red-fg\">                 </span>_mod <span class=\"ansi-blue-fg\">=</span> imp<span class=\"ansi-blue-fg\">.</span>load_module<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;_pywrap_tensorflow_internal&#39;</span><span class=\"ansi-blue-fg\">,</span> fp<span class=\"ansi-blue-fg\">,</span> pathname<span class=\"ansi-blue-fg\">,</span> description<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     25</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.6/imp.py</span> in <span class=\"ansi-cyan-fg\">load_module</span><span class=\"ansi-blue-fg\">(name, file, filename, details)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    242</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 243</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> load_dynamic<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> filename<span class=\"ansi-blue-fg\">,</span> file<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    244</span>     <span class=\"ansi-green-fg\">elif</span> type_ <span class=\"ansi-blue-fg\">==</span> PKG_DIRECTORY<span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.6/imp.py</span> in <span class=\"ansi-cyan-fg\">load_dynamic</span><span class=\"ansi-blue-fg\">(name, path, file)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    342</span>             name=name, loader=loader, origin=path)\n",
       "<span class=\"ansi-green-fg\">--&gt; 343</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> _load<span class=\"ansi-blue-fg\">(</span>spec<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    344</span> \n",
       "\n",
       "<span class=\"ansi-red-fg\">ImportError</span>: libcublas.so.9.0: cannot open shared object file: No such file or directory\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "<span class=\"ansi-red-fg\">ImportError</span>                               Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-3548884146162514&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span><span class=\"ansi-blue-fg\">()</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> <span class=\"ansi-green-fg\">import</span> random\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> <span class=\"ansi-green-fg\">import</span> logging\n",
       "<span class=\"ansi-green-fg\">---&gt; 10</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">import</span> tensorflow <span class=\"ansi-green-fg\">as</span> tf\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> <span class=\"ansi-green-fg\">from</span> collections <span class=\"ansi-green-fg\">import</span> Counter\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> <span class=\"ansi-green-fg\">import</span> pathlib\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.6/site-packages/tensorflow/__init__.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span><span class=\"ansi-blue-fg\">()</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     20</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     21</span> <span class=\"ansi-red-fg\"># pylint: disable=g-bad-import-order</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 22</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> tensorflow<span class=\"ansi-blue-fg\">.</span>python <span class=\"ansi-green-fg\">import</span> pywrap_tensorflow  <span class=\"ansi-red-fg\"># pylint: disable=unused-import</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     23</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     24</span> <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.6/site-packages/tensorflow/python/__init__.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span><span class=\"ansi-blue-fg\">()</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     47</span> <span class=\"ansi-green-fg\">import</span> numpy <span class=\"ansi-green-fg\">as</span> np\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     48</span> \n",
       "<span class=\"ansi-green-fg\">---&gt; 49</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> tensorflow<span class=\"ansi-blue-fg\">.</span>python <span class=\"ansi-green-fg\">import</span> pywrap_tensorflow\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     50</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     51</span> <span class=\"ansi-red-fg\"># Protocol buffers</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span><span class=\"ansi-blue-fg\">()</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     72</span> <span class=\"ansi-green-fg\">for</span> some common reasons <span class=\"ansi-green-fg\">and</span> solutions<span class=\"ansi-blue-fg\">.</span>  Include the entire stack trace\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     73</span> above this error message when asking for help.&#34;&#34;&#34; % traceback.format_exc()\n",
       "<span class=\"ansi-green-fg\">---&gt; 74</span><span class=\"ansi-red-fg\">   </span><span class=\"ansi-green-fg\">raise</span> ImportError<span class=\"ansi-blue-fg\">(</span>msg<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     75</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     76</span> <span class=\"ansi-red-fg\"># pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">ImportError</span>: Traceback (most recent call last):\n",
       "  File &#34;/databricks/python/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py&#34;, line 58, in &lt;module&gt;\n",
       "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
       "  File &#34;/databricks/python/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py&#34;, line 28, in &lt;module&gt;\n",
       "    _pywrap_tensorflow_internal = swig_import_helper()\n",
       "  File &#34;/databricks/python/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py&#34;, line 24, in swig_import_helper\n",
       "    _mod = imp.load_module(&#39;_pywrap_tensorflow_internal&#39;, fp, pathname, description)\n",
       "  File &#34;/databricks/python/lib/python3.6/imp.py&#34;, line 243, in load_module\n",
       "    return load_dynamic(name, filename, file)\n",
       "  File &#34;/databricks/python/lib/python3.6/imp.py&#34;, line 343, in load_dynamic\n",
       "    return _load(spec)\n",
       "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n",
       "\n",
       "\n",
       "Failed to load the native TensorFlow runtime.\n",
       "\n",
       "See https://www.tensorflow.org/install/install_sources#common_installation_problems\n",
       "\n",
       "for some common reasons and solutions.  Include the entire stack trace\n",
       "above this error message when asking for help.</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import modeling, optimization, tokenization\n",
    "from run_pretraining import input_fn_builder, model_fn_builder\n",
    "\n",
    "from text_preprocessing import tokenizer_word\n",
    "from language_model_processing import read_raw_data_preprocess_and_save, create_vocab_df\n",
    "from bpe import create_token_vocabulary, get_stats, merge_vocab, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_maps_dir = '/dbfs/mnt/configandvocab'\n",
    "\n",
    "def save_obj(obj, directory, name):\n",
    "    with open(directory / \"{}.pkl\".format(name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name, directory):\n",
    "    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "      \n",
    "      \n",
    "vocab_to_id = load_obj('vocab_to_id', str(language_maps_dir))\n",
    "len(vocab_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modeling, optimization, tokenization\n",
    "\n",
    "testcase = \"Olá isso é mais uma BAGUNCA 😂😂😂\"\n",
    "bert_tokenizer = tokenization.FullTokenizer(language_maps_dir)\n",
    "print(testcase)\n",
    "print(bert_tokenizer.tokenize(testcase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "bert_base_config = {\n",
    "  \"attention_probs_dropout_prob\": 0.1, \n",
    "  \"directionality\": \"bidi\", \n",
    "  \"hidden_act\": \"gelu\", \n",
    "  \"hidden_dropout_prob\": 0.1, \n",
    "  \"hidden_size\": 768, \n",
    "  \"initializer_range\": 0.02, \n",
    "  \"intermediate_size\": 3072, \n",
    "  \"max_position_embeddings\": 512, \n",
    "  \"num_attention_heads\": 12, \n",
    "  \"num_hidden_layers\": 8, \n",
    "  \"pooler_fc_size\": 768, \n",
    "  \"pooler_num_attention_heads\": 12, \n",
    "  \"pooler_num_fc_layers\": 3, \n",
    "  \"pooler_size_per_head\": 128, \n",
    "  \"pooler_type\": \"first_token_transform\", \n",
    "  \"type_vocab_size\": 2, \n",
    "  \"vocab_size\": len(vocab_to_id)\n",
    "}\n",
    "\n",
    "with open(os.path.join(language_maps_dir, 'bert_config.json'), 'w') as f:\n",
    "    json.dump(bert_base_config, f)\n",
    "    \n",
    "print(bert_base_config)\n",
    "####################################load_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/dbfs/tmp/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename('/dbfs/tmp/model/checkpoint.tmp2feb8d7a932249e7ba1a11f96d3cb334', '/dbfs/tmp/model/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir = '/dbfs/tmp/model'\n",
    "if os.path.exists(dir):\n",
    "    shutil.rmtree(dir)\n",
    "os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
    "SAVE_CHECKPOINTS_STEPS = 250 #@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "model_weights_dir = '/dbfs/tmp/modelweights'\n",
    "pretraining_data_dir = '/dbfs/mnt/pretrainingbasedata'\n",
    "\n",
    "VOCAB_FILE = language_maps_dir + '/vocab_file.csv'\n",
    "CONFIG_FILE = language_maps_dir + '/bert_config.json'\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(model_weights_dir)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.gfile.Glob(os.path.join(pretraining_data_dir,'*tfrecord'))\n",
    "\n",
    "USE_TPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True)\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    model_dir=model_weights_dir,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    keep_checkpoint_max=5,\n",
    "    keep_checkpoint_every_n_hours=1,\n",
    "    log_step_count_steps=100)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)\n",
    "  \n",
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "BERT Train",
  "notebookId": 3548884146162509,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
